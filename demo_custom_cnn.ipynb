{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "## model design: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-build-a-convnet-for-cifar-10-and-cifar-100-classification-with-keras.md\n",
    "import infrastructure as inf\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch    \n",
    "\n",
    "import torch as ch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from fastargs import get_current_config, Param, Section\n",
    "from fastargs.decorators import param\n",
    "from fastargs.validation import And, OneOf\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = inf.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model in PyTorch\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, no_classes):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3)) # output shape: \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, no_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def generate_model(output_dim=100):\n",
    "    model = MyCNNModel(output_dim)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, lr=0.1, epochs=100, momentum=0.9, weight_decay=0.0001,reduce_patience=5, reduce_factor=0.2,tracking_freq=5,do_tracking=True,verbose=True):\n",
    "    \n",
    "    # dictionary to keep track of training params and results\n",
    "    train_dict = {}\n",
    "    train_dict['lr'] = lr\n",
    "    train_dict['epochs'] = epochs\n",
    "    train_dict['momentum'] = momentum\n",
    "    train_dict['weight_decay'] = weight_decay\n",
    "    train_dict['reduce_patience'] = reduce_patience\n",
    "    train_dict['reduce_factor'] = reduce_factor\n",
    "    # results\n",
    "    # training loss is tracked every epoch\n",
    "    train_dict['train_loss'] = []\n",
    "    \n",
    "    # all other params are tracked every e.g. 10 epochs\t(tracking_freq) if do_tracking is True\n",
    "    train_dict['train_acc_top1'] = []\n",
    "    train_dict['train_acc_top5'] = []\n",
    "    train_dict['val_acc_top1'] = []\n",
    "    train_dict['val_acc_top5'] = []\n",
    "    \n",
    "    \n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = ch.nn.CrossEntropyLoss() # doesn't require the input to be in valid probabilities format\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=reduce_patience, verbose=True, factor=reduce_factor)\n",
    "    len_train_loader = len(loaders['train'])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        #for ims, labs in tqdm(loaders['train']):\n",
    "        for ims, labs in loaders['train']:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                loss = criterion(out, labs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step(running_loss)\n",
    "        # save training loss\n",
    "        print(f'Epoch {i+1}/{epochs}, Loss: {running_loss/len_train_loader}')\n",
    "        train_dict['train_loss'].append(running_loss/len_train_loader)\n",
    "        # keep track of other metrics\n",
    "        if do_tracking and (i+1)%tracking_freq == 0:\n",
    "            train_top1, train_top5, val_top1, val_top5 = evaluate(model, loaders, lr_tta=False,verbose=verbose)\n",
    "            train_dict['train_acc_top1'].append(train_top1)\n",
    "            train_dict['train_acc_top5'].append(train_top5)\n",
    "            train_dict['val_acc_top1'].append(val_top1)\n",
    "            train_dict['val_acc_top5'].append(val_top5)\n",
    "    return model, train_dict\n",
    "\n",
    "def evaluate(model, loaders, lr_tta=False,verbose=True):\n",
    "    # lr_tta: whether to use test-time augmentation by flipping images horizontally\n",
    "    model.eval()\n",
    "    train_top1, train_top5, val_top1, val_top5 = 0., 0., 0., 0.\n",
    "    with ch.no_grad():\n",
    "        for name in ['train', 'test']:\n",
    "            total_correct, total_num, total_correct_top5 = 0., 0., 0.\n",
    "            for ims, labs in loaders[name]:\n",
    "                with autocast():\n",
    "                    out = model(ims)\n",
    "                    if lr_tta:\n",
    "                        out += model(ims.flip(-1))\n",
    "                    # computing top1 accuracy\n",
    "                    total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                    total_num += ims.shape[0]\n",
    "                    # computing top5 accuracy\n",
    "                    total_correct_top5 += out.argsort(1)[:,-5:].eq(labs.unsqueeze(-1)).sum().cpu().item()\n",
    "            if verbose:\n",
    "                print(f'{name} (acc) top-1: {total_correct / total_num * 100:.1f}, top-5: {total_correct_top5 / total_num * 100:.1f} %')\n",
    "            if name == 'train':\n",
    "                train_top1, train_top5 = total_correct / total_num * 100, total_correct_top5 / total_num * 100\n",
    "            else:\n",
    "                val_top1, val_top5 = total_correct / total_num * 100, total_correct_top5 / total_num * 100\n",
    "    return train_top1, train_top5, val_top1, val_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders, start_time = inf.make_dataloaders(batch_size=256, num_workers=12)\n",
    "model = generate_model()\n",
    "print(model)\n",
    "# load model from checkpoint stored at ./models/model.pt\n",
    "#model.load_state_dict(torch.load(\"./models/model.pt\"))\n",
    "model, tracked_params = train(model, loaders,epochs=80,tracking_freq=5,reduce_factor=0.2,do_tracking=True,verbose=True)\n",
    "print(f'Total time: {time.time() - start_time:.5f}')\n",
    "evaluate(model, loaders)\n",
    "\n",
    "# store the model   \n",
    "torch.save(model.state_dict(), \"./models/model.pt\")\t\n",
    "# save the tracked params\n",
    "np.save(\"./models/tracked_params.npy\", tracked_params)\n",
    "\n",
    "# visualize the tracked params from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.281559793572677\n",
      "Epoch 2/20, Loss: 2.2788657640155994\n",
      "Epoch 3/20, Loss: 2.258342240986071\n",
      "Epoch 4/20, Loss: 2.048417881915444\n",
      "Epoch 5/20, Loss: 1.9063385411312705\n",
      "train (acc) top-1: 26.7, top-5: 84.7 %\n",
      "test (acc) top-1: 25.7, top-5: 85.1 %\n",
      "Epoch 6/20, Loss: 1.8245369446905035\n",
      "Epoch 7/20, Loss: 1.7192870503977726\n",
      "Epoch 8/20, Loss: 1.663450379120676\n",
      "Epoch 9/20, Loss: 1.6124906790883917\n",
      "Epoch 10/20, Loss: 1.598766421016894\n",
      "train (acc) top-1: 43.4, top-5: 90.7 %\n",
      "test (acc) top-1: 42.1, top-5: 90.5 %\n",
      "Epoch 11/20, Loss: 1.550866911285802\n",
      "Epoch 12/20, Loss: 1.5248469239787052\n",
      "Epoch 13/20, Loss: 1.4818741710562455\n",
      "Epoch 14/20, Loss: 1.4572236600675081\n",
      "Epoch 15/20, Loss: 1.4273042427866083\n",
      "train (acc) top-1: 49.8, top-5: 93.5 %\n",
      "test (acc) top-1: 48.7, top-5: 92.1 %\n",
      "Epoch 16/20, Loss: 1.3779754262221486\n",
      "Epoch 17/20, Loss: 1.3585858345031738\n",
      "Epoch 18/20, Loss: 1.3211899933062101\n",
      "Epoch 19/20, Loss: 1.2716670161799382\n",
      "Epoch 20/20, Loss: 1.339365043138203\n",
      "train (acc) top-1: 52.2, top-5: 94.2 %\n",
      "test (acc) top-1: 48.7, top-5: 92.3 %\n",
      "Total time: 13.79085\n",
      "train (acc) top-1: 51.9, top-5: 94.4 %\n",
      "test (acc) top-1: 48.7, top-5: 92.3 %\n",
      "Epoch 1/20, Loss: 2.23657909192537\n",
      "Epoch 2/20, Loss: 2.213151316893728\n",
      "Epoch 3/20, Loss: 1.752457530874955\n",
      "Epoch 4/20, Loss: 1.567351680052908\n",
      "Epoch 5/20, Loss: 1.474366501757973\n",
      "train (acc) top-1: 41.9, top-5: 96.0 %\n",
      "test (acc) top-1: 41.1, top-5: 96.3 %\n",
      "Epoch 6/20, Loss: 1.407098412513733\n",
      "Epoch 7/20, Loss: 1.3912218244452226\n",
      "Epoch 8/20, Loss: 1.3917876419268156\n",
      "Epoch 9/20, Loss: 1.2746930624309338\n",
      "Epoch 10/20, Loss: 1.241986073945698\n",
      "train (acc) top-1: 50.3, top-5: 97.3 %\n",
      "test (acc) top-1: 51.8, top-5: 98.0 %\n",
      "Epoch 11/20, Loss: 1.251884008708753\n",
      "Epoch 12/20, Loss: 1.2170990956457037\n",
      "Epoch 13/20, Loss: 1.1856239532169544\n",
      "Epoch 14/20, Loss: 1.1911590977718955\n",
      "Epoch 15/20, Loss: 1.1645606693468595\n",
      "train (acc) top-1: 53.2, top-5: 98.5 %\n",
      "test (acc) top-1: 52.8, top-5: 98.4 %\n",
      "Epoch 16/20, Loss: 1.1450163627925671\n",
      "Epoch 17/20, Loss: 1.0775710783506696\n",
      "Epoch 18/20, Loss: 1.08614939137509\n",
      "Epoch 19/20, Loss: 1.063759850828271\n",
      "Epoch 20/20, Loss: 1.0428714030667354\n",
      "train (acc) top-1: 58.8, top-5: 99.0 %\n",
      "test (acc) top-1: 56.9, top-5: 98.5 %\n",
      "Total time: 14.42606\n",
      "train (acc) top-1: 58.9, top-5: 98.8 %\n",
      "test (acc) top-1: 56.9, top-5: 98.5 %\n",
      "Epoch 1/20, Loss: 2.27787278827868\n",
      "Epoch 2/20, Loss: 2.278620017202277\n",
      "Epoch 3/20, Loss: 2.2076479259290194\n",
      "Epoch 4/20, Loss: 2.0687801461470756\n",
      "Epoch 5/20, Loss: 2.0000233901174447\n",
      "train (acc) top-1: 25.6, top-5: 80.6 %\n",
      "test (acc) top-1: 25.1, top-5: 77.6 %\n",
      "Epoch 6/20, Loss: 1.9065536197863127\n",
      "Epoch 7/20, Loss: 1.843701249674747\n",
      "Epoch 8/20, Loss: 1.8843187532926862\n",
      "Epoch 9/20, Loss: 1.7648756315833645\n",
      "Epoch 10/20, Loss: 1.7188130052466142\n",
      "train (acc) top-1: 38.9, top-5: 89.4 %\n",
      "test (acc) top-1: 37.6, top-5: 87.7 %\n",
      "Epoch 11/20, Loss: 1.6503068961595233\n",
      "Epoch 12/20, Loss: 1.577583532584341\n",
      "Epoch 13/20, Loss: 1.5069271451548527\n",
      "Epoch 14/20, Loss: 1.468414118415431\n",
      "Epoch 15/20, Loss: 1.4011287124533403\n",
      "train (acc) top-1: 48.6, top-5: 94.7 %\n",
      "test (acc) top-1: 49.6, top-5: 94.1 %\n",
      "Epoch 16/20, Loss: 1.3377147787495662\n",
      "Epoch 17/20, Loss: 1.3720495951803107\n",
      "Epoch 18/20, Loss: 1.2833517099681653\n",
      "Epoch 19/20, Loss: 1.3036273655138517\n",
      "Epoch 20/20, Loss: 1.2332913561871177\n",
      "train (acc) top-1: 54.9, top-5: 95.3 %\n",
      "test (acc) top-1: 54.6, top-5: 94.2 %\n",
      "Total time: 13.81248\n",
      "train (acc) top-1: 55.6, top-5: 95.4 %\n",
      "test (acc) top-1: 54.6, top-5: 94.2 %\n",
      "Epoch 1/20, Loss: 2.27413579037315\n",
      "Epoch 2/20, Loss: 2.3151284644478247\n",
      "Epoch 3/20, Loss: 2.2965520557604338\n",
      "Epoch 4/20, Loss: 2.298176414088199\n",
      "Epoch 5/20, Loss: 2.2110823455609774\n",
      "train (acc) top-1: 9.9, top-5: 49.8 %\n",
      "test (acc) top-1: 10.0, top-5: 49.6 %\n",
      "Epoch 6/20, Loss: 2.2899467066714636\n",
      "Epoch 7/20, Loss: 2.1247662494057105\n",
      "Epoch 8/20, Loss: 1.9736828992241306\n",
      "Epoch 9/20, Loss: 1.7413411265925358\n",
      "Epoch 10/20, Loss: 1.7578131838848716\n",
      "train (acc) top-1: 30.8, top-5: 94.3 %\n",
      "test (acc) top-1: 31.9, top-5: 95.1 %\n",
      "Epoch 11/20, Loss: 1.6966285140890824\n",
      "Epoch 12/20, Loss: 1.5689708559136641\n",
      "Epoch 13/20, Loss: 1.5052877539082576\n",
      "Epoch 14/20, Loss: 1.4509742259979248\n",
      "Epoch 15/20, Loss: 1.4255018987153705\n",
      "train (acc) top-1: 41.7, top-5: 94.4 %\n",
      "test (acc) top-1: 40.4, top-5: 92.8 %\n",
      "Epoch 16/20, Loss: 1.3974154936639887\n",
      "Epoch 17/20, Loss: 1.3622973278949135\n",
      "Epoch 18/20, Loss: 1.3156430972249884\n",
      "Epoch 19/20, Loss: 1.3225756883621216\n",
      "Epoch 20/20, Loss: 1.2696291772942794\n",
      "train (acc) top-1: 52.0, top-5: 97.9 %\n",
      "test (acc) top-1: 53.6, top-5: 98.0 %\n",
      "Total time: 13.93543\n",
      "train (acc) top-1: 51.5, top-5: 97.9 %\n",
      "test (acc) top-1: 53.6, top-5: 98.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-245:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/ffcv/loader/epoch_iterator.py\", line 84, in run\n",
      "    result = self.run_pipeline(b_ix, ixes, slot, events[slot])\n",
      "  File \"/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/ffcv/loader/epoch_iterator.py\", line 146, in run_pipeline\n",
      "    results = stage_code(**args)\n",
      "  File \"\", line 2, in stage_code_1\n",
      "  File \"/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/ffcv/transforms/ops.py\", line 91, in to_torch_image\n",
      "    assert inp.is_contiguous(memory_format=ch.channels_last)\n",
      "AssertionError\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m loaders, start_time \u001b[39m=\u001b[39m inf\u001b[39m.\u001b[39mmake_dataloaders(paths[\u001b[39m0\u001b[39m],paths[\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m generate_model(\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model, tracked_params \u001b[39m=\u001b[39m train(model, loaders,epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,tracking_freq\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,reduce_factor\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,do_tracking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal time: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m evaluate(model, loaders)\n",
      "\u001b[1;32m/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#for ims, labs in tqdm(loaders['train']):\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m ims, labs \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mwith\u001b[39;00m autocast():\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/ffcv/loader/epoch_iterator.py:155\u001b[0m, in \u001b[0;36mEpochIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_queue\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do training on the models for the tupels of superclasses\n",
    "import gc\n",
    "for i in range(0,20):\n",
    "    for j in range(i+1,20):\n",
    "        paths =  [f'./data/subsets/{dataset_name}_superclass_{i}_{j}.beton' for dataset_name in [\"train\",\"test\"]]\n",
    "        loaders, start_time = inf.make_dataloaders(paths[0],paths[1])\n",
    "        model = generate_model(10)\n",
    "        model, tracked_params = train(model, loaders,epochs=20,tracking_freq=5,reduce_factor=0.2,do_tracking=True,verbose=True)\n",
    "        print(f'Total time: {time.time() - start_time:.5f}')\n",
    "        evaluate(model, loaders)\n",
    "        # store the model   \n",
    "        torch.save(model.state_dict(), f'./models/model_{i}_{j}.pt')\t\n",
    "        # save the tracked params\n",
    "        np.save(f\"./models/tracked_params{i}_{j}.npy\", tracked_params)\n",
    "        \n",
    "        # once done remove the model, tracked params and loaders from storage\n",
    "        \n",
    "        del model, tracked_params, loaders, start_time\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
