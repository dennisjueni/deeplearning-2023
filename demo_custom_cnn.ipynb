{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "## model design: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-build-a-convnet-for-cifar-10-and-cifar-100-classification-with-keras.md\n",
    "import infrastructure as inf\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch    \n",
    "\n",
    "import torch as ch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from fastargs import get_current_config, Param, Section\n",
    "from fastargs.decorators import param\n",
    "from fastargs.validation import And, OneOf\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = inf.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN model in PyTorch\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, no_classes):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3)) # output shape: \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, no_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def generate_model(output_dim=100):\n",
    "    model = MyCNNModel(output_dim)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loaders, lr=0.1, epochs=100, momentum=0.9, weight_decay=0.0001, reduce_patience=5, reduce_factor=0.2, tracking_freq=5,early_stopping_patience=10, early_stopping_min_epochs=100, do_tracking=True, verbose=False):\n",
    "    # dictionary to keep track of training params and results\n",
    "    train_dict = {}\n",
    "    train_dict['lr'] = lr\n",
    "    train_dict['epochs'] = epochs\n",
    "    train_dict['momentum'] = momentum\n",
    "    train_dict['weight_decay'] = weight_decay\n",
    "    train_dict['reduce_patience'] = reduce_patience\n",
    "    train_dict['reduce_factor'] = reduce_factor\n",
    "    train_dict['tracking_freq'] = tracking_freq\n",
    "    # results\n",
    "    # training loss is tracked every epoch\n",
    "    train_dict['train_loss'] = []\n",
    "    train_dict['val_loss'] = []\n",
    "    train_dict['lr_list'] = []\n",
    "    train_dict['train_acc_top1'] = []\n",
    "    train_dict['train_acc_top5'] = []\n",
    "    train_dict['val_acc_top1'] = []\n",
    "    train_dict['val_acc_top5'] = []\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = ch.nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=reduce_patience, verbose=True, factor=reduce_factor)\n",
    "    len_train_loader = len(loaders['train'])\n",
    "    len_val_loader = len(loaders['test'])\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for i in tqdm(range(epochs),disable=verbose):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_correct, total_num, total_correct_top5 = 0., 0., 0.\n",
    "\n",
    "        for ims, labs in loaders['train']:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                loss = criterion(out, labs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if do_tracking and (i+1)%tracking_freq == 0: # only do bookkeeping if needed\n",
    "                # computing top1 accuracy\n",
    "                total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                total_num += ims.shape[0]\n",
    "                # computing top5 accuracy\n",
    "                total_correct_top5 += out.argsort(1)[:,-5:].eq(labs.unsqueeze(-1)).sum().cpu().item()\n",
    "\n",
    "        scheduler.step(running_loss)\n",
    "        # save training loss\n",
    "        if verbose: print(f'Epoch {i+1}/{epochs}, Training Loss: {running_loss/len_train_loader}')\n",
    "        train_dict['train_loss'].append(running_loss/len_train_loader)\n",
    "        # keep track of the current lr \n",
    "        train_dict['lr_list'].append(optimizer.param_groups[0]['lr'])\n",
    "        # keep track of other metrics\n",
    "        if do_tracking and (i+1)%tracking_freq == 0:\n",
    "            train_top1 = total_correct / total_num * 100\n",
    "            train_top5 = total_correct_top5 / total_num * 100\n",
    "            val_loss = 0.0\n",
    "            total_val_correct, total_val_num, total_val_correct_top5 = 0., 0., 0.\n",
    "            model.eval()\n",
    "            with ch.no_grad():\n",
    "                for val_ims, val_labs in loaders['test']:\n",
    "                    val_out = model(val_ims)\n",
    "                    val_loss += criterion(val_out, val_labs).item()\n",
    "                    # computing top1 accuracy\n",
    "                    total_val_correct += val_out.argmax(1).eq(val_labs).sum().cpu().item()\n",
    "                    total_val_num += val_ims.shape[0]\n",
    "                    # computing top5 accuracy\n",
    "                    total_val_correct_top5 += val_out.argsort(1)[:,-5:].eq(val_labs.unsqueeze(-1)).sum().cpu().item()\n",
    "            val_loss /= len_val_loader\n",
    "            val_top1 = total_val_correct / total_val_num * 100\n",
    "            val_top5 = total_val_correct_top5 / total_val_num * 100\n",
    "            train_dict['val_loss'].append(val_loss)\n",
    "            train_dict['train_acc_top1'].append(train_top1)\n",
    "            train_dict['train_acc_top5'].append(train_top5)\n",
    "            train_dict['val_acc_top1'].append(val_top1)\n",
    "            train_dict['val_acc_top5'].append(val_top5)\n",
    "            if verbose: print(f'Epoch {i+1}/{epochs}, Validation Loss: {val_loss}')\n",
    "            if i > early_stopping_min_epochs:\n",
    "                # Early stopping based on increasing validation loss\n",
    "                if val_loss > best_val_loss:\n",
    "                    early_stopping_counter += 1\n",
    "                    if early_stopping_counter >= early_stopping_patience:\n",
    "                        print(f\"Early stopping triggered at epoch {i}!\")\n",
    "                        return model, train_dict\n",
    "                else:\n",
    "                    best_val_loss = val_loss\n",
    "                    early_stopping_counter = 0\n",
    "\n",
    "    return model, train_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNModel(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 54/150 [01:20<02:02,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00054: reducing learning rate of group 0 to 2.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 111/150 [02:43<00:57,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 111!\n",
      "Total time: 164.33727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaders, start_time = inf.make_dataloaders(batch_size=256, num_workers=12)\n",
    "model = generate_model()\n",
    "print(model)\n",
    "# load model from checkpoint stored at ./models/model.pt\n",
    "#model.load_state_dict(torch.load(\"./models/model.pt\"))\n",
    "model, tracked_params = train(model, loaders,epochs=150,tracking_freq=2,reduce_factor=0.2,early_stopping_min_epochs=100,early_stopping_patience=5,do_tracking=True,verbose=False)\n",
    "print(f'Total time: {time.time() - start_time:.5f}')\n",
    "#evaluate(model, loaders)\n",
    "\n",
    "# store the model   \n",
    "torch.save(model.state_dict(), \"./models/model.pt\")\t\n",
    "# save the tracked params\n",
    "np.save(\"./models/tracked_params.npy\", tracked_params)\n",
    "\n",
    "# visualize the tracked params from training\n",
    "inf.plot_training(tracked_params,\"whole_cifar100\",plot=False,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.121706GB\n",
      "torch.cuda.memory_reserved: 0.556641GB\n",
      "torch.cuda.max_memory_reserved: 0.556641GB\n",
      "Training model for superclasses 0 and 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:33,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.48851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 62/140 [00:21<00:14,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00062: reducing learning rate of group 0 to 2.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 77/140 [00:24<00:19,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 77!\n",
      "Total time: 24.49734\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 55/140 [00:20<00:31,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 55!\n",
      "Total time: 20.18290\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.62774\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 59/140 [00:21<00:28,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 59!\n",
      "Total time: 21.20102\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.64749\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 55/140 [00:20<00:31,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 55!\n",
      "Total time: 20.77265\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 55/140 [00:20<00:31,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 55!\n",
      "Total time: 20.32211\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 59/140 [00:21<00:29,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 59!\n",
      "Total time: 21.47612\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.70109\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:34,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 20.40465\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 51/140 [00:20<00:35,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 20.28369\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 53/140 [00:19<00:32,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 53!\n",
      "Total time: 20.11522\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:33,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.56097\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.160156GB\n",
      "Training model for superclasses 0 and 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 51/140 [00:19<00:34,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.63516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 0 and 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:20<00:35,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 20.44118\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 0 and 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 51/140 [00:20<00:35,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 20.40308\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 0 and 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 51/140 [00:20<00:35,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 20.27091\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 0 and 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 53/140 [00:20<00:33,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 53!\n",
      "Total time: 20.47783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 1 and 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 51/140 [00:19<00:34,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51!\n",
      "Total time: 19.87965\n",
      "torch.cuda.memory_allocated: 0.296266GB\n",
      "torch.cuda.memory_reserved: 0.689453GB\n",
      "torch.cuda.max_memory_reserved: 1.164062GB\n",
      "Training model for superclasses 1 and 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/140 [00:11<08:29,  3.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m loaders, start_time \u001b[39m=\u001b[39m inf\u001b[39m.\u001b[39mmake_dataloaders(paths[\u001b[39m0\u001b[39m],paths[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m generate_model(\u001b[39m10\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model, tracked_params \u001b[39m=\u001b[39m train(model, loaders,epochs\u001b[39m=\u001b[39;49m\u001b[39m140\u001b[39;49m,tracking_freq\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,reduce_factor\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,reduce_patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,do_tracking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,early_stopping_min_epochs\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,early_stopping_patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal time: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# store the model   \u001b[39;00m\n",
      "\u001b[1;32m/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m total_correct, total_num, total_correct_top5 \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m ims, labs \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad(set_to_none\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/janulm/Documents/ETH/SM9/DeepLearning/deep-learning-advanced-initialization/demo_custom_cnn.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m         out \u001b[39m=\u001b[39m model(ims)\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torch/optim/optimizer.py:803\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    804\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    805\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torch/autograd/profiler.py:631\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do training on the models for the tupels of superclasses\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "import gc\n",
    "for i in range(0,20):\n",
    "    for j in range(i+1,20):\n",
    "        \n",
    "        print(f\"Training model for superclasses {i} and {j}\")\n",
    "        paths =  [f'./data/subsets/{dataset_name}_superclass_{i}_{j}.beton' for dataset_name in [\"train\",\"test\"]]\n",
    "        loaders, start_time = inf.make_dataloaders(paths[0],paths[1])\n",
    "        model = generate_model(10)\n",
    "        model, tracked_params = train(model, loaders,epochs=140,tracking_freq=2,reduce_factor=0.2,reduce_patience=5,do_tracking=True,early_stopping_min_epochs=40,early_stopping_patience=5,verbose=False)\n",
    "        print(f'Total time: {time.time() - start_time:.5f}')\n",
    "        # store the model   \n",
    "        torch.save(model.state_dict(), f'./models/model_{i}_{j}.pt')\t\n",
    "        # save the tracked params\n",
    "        np.save(f\"./models/tracked_params{i}_{j}.npy\", tracked_params)\n",
    "        \n",
    "        # once done remove the model, tracked params and loaders from storage\n",
    "        name = f'model_{i}_{j}'\n",
    "        inf.plot_training(tracked_params,name, False, True)\n",
    "        del model, tracked_params, loaders, start_time\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "        print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "        print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
