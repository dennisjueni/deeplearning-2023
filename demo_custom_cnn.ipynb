{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model design: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-build-a-convnet-for-cifar-10-and-cifar-100-classification-with-keras.md\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch    \n",
    "\n",
    "import torch as ch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from fastargs import get_current_config, Param, Section\n",
    "from fastargs.decorators import param\n",
    "from fastargs.validation import And, OneOf\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device {device}')\n",
    "\n",
    "def load_cifar100(train_dataset=\"./data/cifar_train.beton\", val_dataset=\"./data/cifar_test.beton\"):\n",
    "    datasets = {\n",
    "        'train': torchvision.datasets.CIFAR100('./data', train=True, download=True),\n",
    "        'test': torchvision.datasets.CIFAR100('./data', train=False, download=True)\n",
    "        }\n",
    "\n",
    "    for (name, ds) in datasets.items():\n",
    "        path = train_dataset if name == 'train' else val_dataset\n",
    "        writer = DatasetWriter(path, {\n",
    "            'image': RGBImageField(),\n",
    "            'label': IntField()\n",
    "        })\n",
    "        writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR100\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 35355.87it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 16564.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# making the dataloaders for all the tuples of two superclasses: \n",
    "# i.e. first writing these datasets to disk, then loading them into memory using the classical dataloader\n",
    "\n",
    "def load_cifar100_splits(train_dataset=\"./data/cifar_train_idx.beton\", val_dataset=\"./data/cifar_test_idx.beton\"):\n",
    "    datasets = {\n",
    "        'train': torchvision.datasets.CIFAR100('./data', train=True, download=True),\n",
    "        'test': torchvision.datasets.CIFAR100('./data', train=False, download=True)\n",
    "        }\n",
    "    \n",
    "    # get the superclass groups\n",
    "    print(datasets['train'])\n",
    "\n",
    "    for (name, ds) in datasets.items():\n",
    "        path = train_dataset if name == 'train' else val_dataset\n",
    "        writer = DatasetWriter(path, {\n",
    "            'image': RGBImageField(),\n",
    "            'label': IntField()\n",
    "        })\n",
    "        writer.from_indexed_dataset(ds)\n",
    "\n",
    "load_cifar100_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(train_dataset=\"./data/cifar_train.beton\", val_dataset=\"./data/cifar_test.beton\", batch_size=256, num_workers=12):\n",
    "    paths = {\n",
    "        'train': train_dataset,\n",
    "        'test': val_dataset\n",
    "\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    # computed these values earlier and hardcoded them here\t\n",
    "    CIFAR_MEAN = [129.310, 124.108, 112.404]\n",
    "    CIFAR_STD = [68.2125, 65.4075, 70.4055]\n",
    "    loaders = {}\n",
    "\n",
    "    for name in ['train', 'test']:\n",
    "        label_pipeline: List[Operation] = [IntDecoder(), ToTensor(), ToDevice(ch.device(device)), Squeeze()]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "        if name == 'train':\n",
    "            image_pipeline.extend([\n",
    "                RandomHorizontalFlip(), # flips the image horizontally with a probability of 0.5\n",
    "                RandomTranslate(padding=2, fill=tuple(map(int, CIFAR_MEAN))), # shifts the image horizontally and vertically by a random amount\n",
    "                Cutout(4, tuple(map(int, CIFAR_MEAN))), # sets a random square patch of the image to mean\n",
    "            ])\n",
    "        image_pipeline.extend([\n",
    "            ToTensor(),\n",
    "            ToDevice(ch.device(device), non_blocking=True),\n",
    "            ToTorchImage(),\n",
    "            Convert(ch.float32), # TODO check what the impact for float16 is (it was the initial value, and why it crashes with float16)\t\n",
    "        ])\n",
    "        if name == 'train':\n",
    "            image_pipeline.extend([\n",
    "                v2.RandomRotation(15),\n",
    "            ])\n",
    "        image_pipeline.extend([\n",
    "            v2.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "        \n",
    "        ordering = OrderOption.RANDOM if name == 'train' else OrderOption.SEQUENTIAL\n",
    "\n",
    "        loaders[name] = Loader(paths[name], batch_size=batch_size, num_workers=num_workers,\n",
    "                               order=ordering, drop_last=(name == 'train'),\n",
    "                               pipelines={'image': image_pipeline, 'label': label_pipeline})\n",
    "\n",
    "    return loaders, start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model in PyTorch\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, no_classes):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3)) # output shape: \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, no_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def generate_model(output_dim=100):\n",
    "    model = MyCNNModel(output_dim)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, lr=0.1, epochs=100, momentum=0.9, weight_decay=0.0001,reduce_patience=5, reduce_factor=0.2,tracking_freq=5,do_tracking=True,verbose=True):\n",
    "    \n",
    "    # dictionary to keep track of training params and results\n",
    "    train_dict = {}\n",
    "    train_dict['lr'] = lr\n",
    "    train_dict['epochs'] = epochs\n",
    "    train_dict['momentum'] = momentum\n",
    "    train_dict['weight_decay'] = weight_decay\n",
    "    train_dict['reduce_patience'] = reduce_patience\n",
    "    train_dict['reduce_factor'] = reduce_factor\n",
    "    # results\n",
    "    # training loss is tracked every epoch\n",
    "    train_dict['train_loss'] = []\n",
    "    \n",
    "    # all other params are tracked every e.g. 10 epochs\t(tracking_freq) if do_tracking is True\n",
    "    train_dict['train_acc_top1'] = []\n",
    "    train_dict['train_acc_top5'] = []\n",
    "    train_dict['val_acc_top1'] = []\n",
    "    train_dict['val_acc_top5'] = []\n",
    "    \n",
    "    \n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = ch.nn.CrossEntropyLoss() # doesn't require the input to be in valid probabilities format\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=reduce_patience, verbose=True, factor=reduce_factor)\n",
    "    len_train_loader = len(loaders['train'])\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        #for ims, labs in tqdm(loaders['train']):\n",
    "        for ims, labs in loaders['train']:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                loss = criterion(out, labs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step(running_loss)\n",
    "        # save training loss\n",
    "        print(f'Epoch {i+1}/{epochs}, Loss: {running_loss/len_train_loader}')\n",
    "        train_dict['train_loss'].append(running_loss/len_train_loader)\n",
    "        # keep track of other metrics\n",
    "        if do_tracking and (i+1)%tracking_freq == 0:\n",
    "            train_top1, train_top5, val_top1, val_top5 = evaluate(model, loaders, lr_tta=False,verbose=verbose)\n",
    "            train_dict['train_acc_top1'].append(train_top1)\n",
    "            train_dict['train_acc_top5'].append(train_top5)\n",
    "            train_dict['val_acc_top1'].append(val_top1)\n",
    "            train_dict['val_acc_top5'].append(val_top5)\n",
    "    return model, train_dict\n",
    "\n",
    "def evaluate(model, loaders, lr_tta=False,verbose=True):\n",
    "    # lr_tta: whether to use test-time augmentation by flipping images horizontally\n",
    "    model.eval()\n",
    "    train_top1, train_top5, val_top1, val_top5 = 0., 0., 0., 0.\n",
    "    with ch.no_grad():\n",
    "        for name in ['train', 'test']:\n",
    "            total_correct, total_num, total_correct_top5 = 0., 0., 0.\n",
    "            for ims, labs in loaders[name]:\n",
    "                with autocast():\n",
    "                    out = model(ims)\n",
    "                    if lr_tta:\n",
    "                        out += model(ims.flip(-1))\n",
    "                    # computing top1 accuracy\n",
    "                    total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                    total_num += ims.shape[0]\n",
    "                    # computing top5 accuracy\n",
    "                    total_correct_top5 += out.argsort(1)[:,-5:].eq(labs.unsqueeze(-1)).sum().cpu().item()\n",
    "            if verbose:\n",
    "                print(f'{name} (acc) top-1: {total_correct / total_num * 100:.1f}, top-5: {total_correct_top5 / total_num * 100:.1f} %')\n",
    "            if name == 'train':\n",
    "                train_top1, train_top5 = total_correct / total_num * 100, total_correct_top5 / total_num * 100\n",
    "            else:\n",
    "                val_top1, val_top5 = total_correct / total_num * 100, total_correct_top5 / total_num * 100\n",
    "    return train_top1, train_top5, val_top1, val_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 248247.42it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 99563.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNModel(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=100, bias=True)\n",
      ")\n",
      "Epoch 1/80, Loss: 4.239824879475129\n",
      "Epoch 2/80, Loss: 3.6604695772513365\n",
      "Epoch 3/80, Loss: 3.3152123365646755\n",
      "Epoch 4/80, Loss: 3.064133582971035\n",
      "Epoch 5/80, Loss: 2.8968504979060246\n",
      "train (acc) top-1: 30.2, top-5: 62.1 %\n",
      "test (acc) top-1: 30.9, top-5: 62.8 %\n",
      "Epoch 6/80, Loss: 2.7644205655807106\n",
      "Epoch 7/80, Loss: 2.664022786800678\n",
      "Epoch 8/80, Loss: 2.5875365575154623\n",
      "Epoch 9/80, Loss: 2.543903240790734\n",
      "Epoch 10/80, Loss: 2.476747717001499\n",
      "train (acc) top-1: 38.1, top-5: 68.6 %\n",
      "test (acc) top-1: 37.8, top-5: 68.3 %\n",
      "Epoch 11/80, Loss: 2.4454956763829943\n",
      "Epoch 12/80, Loss: 2.411820486264351\n",
      "Epoch 13/80, Loss: 2.3848981356009458\n",
      "Epoch 14/80, Loss: 2.342854288296822\n",
      "Epoch 15/80, Loss: 2.3157739284711005\n",
      "train (acc) top-1: 42.5, top-5: 73.0 %\n",
      "test (acc) top-1: 41.3, top-5: 70.5 %\n",
      "Epoch 16/80, Loss: 2.3053662789173615\n",
      "Epoch 17/80, Loss: 2.2845819363227258\n",
      "Epoch 18/80, Loss: 2.292568759429149\n",
      "Epoch 19/80, Loss: 2.250380001923977\n",
      "Epoch 20/80, Loss: 2.2609390705059735\n",
      "train (acc) top-1: 44.5, top-5: 75.1 %\n",
      "test (acc) top-1: 42.6, top-5: 72.4 %\n",
      "Epoch 21/80, Loss: 2.2276958630635186\n",
      "Epoch 22/80, Loss: 2.2444654000111117\n",
      "Epoch 23/80, Loss: 2.231677222251892\n",
      "Epoch 24/80, Loss: 2.2171104449492236\n",
      "Epoch 25/80, Loss: 2.197204153354351\n",
      "train (acc) top-1: 45.5, top-5: 75.2 %\n",
      "test (acc) top-1: 42.9, top-5: 71.5 %\n",
      "Epoch 26/80, Loss: 2.186193212484702\n",
      "Epoch 27/80, Loss: 2.179836729245308\n",
      "Epoch 28/80, Loss: 2.1642987220715253\n",
      "Epoch 29/80, Loss: 2.165679042155926\n",
      "Epoch 30/80, Loss: 2.173588226391719\n",
      "train (acc) top-1: 45.7, top-5: 75.7 %\n",
      "test (acc) top-1: 43.0, top-5: 71.9 %\n",
      "Epoch 31/80, Loss: 2.1626312787716206\n",
      "Epoch 32/80, Loss: 2.169297480583191\n",
      "Epoch 33/80, Loss: 2.17408843651796\n",
      "Epoch 34/80, Loss: 2.1755125785485294\n",
      "Epoch 35/80, Loss: 2.161842230038765\n",
      "train (acc) top-1: 45.3, top-5: 75.1 %\n",
      "test (acc) top-1: 41.8, top-5: 71.3 %\n",
      "Epoch 36/80, Loss: 2.1460083759748017\n",
      "Epoch 37/80, Loss: 2.1412146054781402\n",
      "Epoch 38/80, Loss: 2.139401978101486\n",
      "Epoch 39/80, Loss: 2.1240128639416818\n",
      "Epoch 40/80, Loss: 2.1329509313289936\n",
      "train (acc) top-1: 46.4, top-5: 76.0 %\n",
      "test (acc) top-1: 42.5, top-5: 71.1 %\n",
      "Epoch 41/80, Loss: 2.1405515022766894\n",
      "Epoch 42/80, Loss: 2.1359067702904726\n",
      "Epoch 43/80, Loss: 2.131601169781807\n",
      "Epoch 44/80, Loss: 2.103095511901073\n",
      "Epoch 45/80, Loss: 2.0950848744465755\n",
      "train (acc) top-1: 48.3, top-5: 77.1 %\n",
      "test (acc) top-1: 45.1, top-5: 72.5 %\n",
      "Epoch 46/80, Loss: 2.096939449432569\n",
      "Epoch 47/80, Loss: 2.099053228818453\n",
      "Epoch 48/80, Loss: 2.090407661902599\n",
      "Epoch 49/80, Loss: 2.0922487809107855\n",
      "Epoch 50/80, Loss: 2.076852566156632\n",
      "train (acc) top-1: 47.9, top-5: 76.9 %\n",
      "test (acc) top-1: 43.9, top-5: 72.5 %\n",
      "Epoch 51/80, Loss: 2.0753622482984495\n",
      "Epoch 52/80, Loss: 2.0947144966859086\n",
      "Epoch 53/80, Loss: 2.0790240275554166\n",
      "Epoch 54/80, Loss: 2.0768029915980804\n",
      "Epoch 55/80, Loss: 2.0720834255218508\n",
      "train (acc) top-1: 48.3, top-5: 77.3 %\n",
      "test (acc) top-1: 44.0, top-5: 72.9 %\n",
      "Epoch 56/80, Loss: 2.0828361022166715\n",
      "Epoch 57/80, Loss: 2.0738166344471467\n",
      "Epoch 58/80, Loss: 2.0805322873286713\n",
      "Epoch 59/80, Loss: 2.051824099589617\n",
      "Epoch 60/80, Loss: 2.047142681097373\n",
      "train (acc) top-1: 48.9, top-5: 77.8 %\n",
      "test (acc) top-1: 43.6, top-5: 72.0 %\n",
      "Epoch 61/80, Loss: 2.07766468219268\n",
      "Epoch 62/80, Loss: 2.044785722096761\n",
      "Epoch 63/80, Loss: 2.0516474216412277\n",
      "Epoch 64/80, Loss: 2.0548171477440076\n",
      "Epoch 65/80, Loss: 2.032209967955565\n",
      "train (acc) top-1: 47.6, top-5: 76.7 %\n",
      "test (acc) top-1: 43.5, top-5: 71.7 %\n",
      "Epoch 66/80, Loss: 2.0656196404726077\n",
      "Epoch 67/80, Loss: 2.0757806019905285\n",
      "Epoch 68/80, Loss: 2.047143868299631\n",
      "Epoch 69/80, Loss: 2.0425785125830234\n",
      "Epoch 70/80, Loss: 2.052643346175169\n",
      "train (acc) top-1: 49.5, top-5: 77.8 %\n",
      "test (acc) top-1: 44.7, top-5: 72.5 %\n",
      "Epoch 00071: reducing learning rate of group 0 to 2.0000e-02.\n",
      "Epoch 71/80, Loss: 2.055529579749474\n",
      "Epoch 72/80, Loss: 1.6622068869761932\n",
      "Epoch 73/80, Loss: 1.51101987300775\n",
      "Epoch 74/80, Loss: 1.4546553367223496\n",
      "Epoch 75/80, Loss: 1.4071993736120372\n",
      "train (acc) top-1: 62.9, top-5: 87.2 %\n",
      "test (acc) top-1: 53.7, top-5: 79.6 %\n",
      "Epoch 76/80, Loss: 1.3590174204263932\n",
      "Epoch 77/80, Loss: 1.3401692060323862\n",
      "Epoch 78/80, Loss: 1.3205196643487\n",
      "Epoch 79/80, Loss: 1.2899845108007773\n",
      "Epoch 80/80, Loss: 1.2711689172646938\n",
      "train (acc) top-1: 65.3, top-5: 88.8 %\n",
      "test (acc) top-1: 53.5, top-5: 79.8 %\n",
      "Total time: 103.78060\n",
      "train (acc) top-1: 65.5, top-5: 89.0 %\n",
      "test (acc) top-1: 53.5, top-5: 79.8 %\n"
     ]
    }
   ],
   "source": [
    "load_cifar100()\n",
    "loaders, start_time = make_dataloaders(batch_size=256, num_workers=12)\n",
    "model = generate_model()\n",
    "print(model)\n",
    "# load model from checkpoint stored at ./models/model.pt\n",
    "#model.load_state_dict(torch.load(\"./models/model.pt\"))\n",
    "model, tracked_params = train(model, loaders,epochs=80,tracking_freq=5,reduce_factor=0.2,do_tracking=True,verbose=True)\n",
    "print(f'Total time: {time.time() - start_time:.5f}')\n",
    "evaluate(model, loaders)\n",
    "\n",
    "# store the model   \n",
    "torch.save(model.state_dict(), \"./models/model.pt\")\t\n",
    "# save the tracked params\n",
    "np.save(\"./models/tracked_params.npy\", tracked_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
