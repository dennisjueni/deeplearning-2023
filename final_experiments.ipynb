{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import infrastructure as inf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch \n",
    "from torchvision.models import resnet18\n",
    "import torchvision\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines the number of models used to average the results in our experiments and the number of subsets of the data we test on\n",
    "num_models_per_experiment = 5\n",
    "num_subsets_to_test = 10\n",
    "\n",
    "# all possible classes with indices between 10 and 20 are used for testing our results, while the models were trained on the first 10 classes\n",
    "possible_tuples = []\n",
    "for i in range(10,20):\n",
    "    for j in range(i+1,20):\n",
    "        possible_tuples.append((i,j))\n",
    "\n",
    "# randomly sample 10 tuples from the list of possible tuples to test on\n",
    "sample_tuples = random.sample(possible_tuples,num_subsets_to_test)\n",
    "# sort the sampled tuples\n",
    "sample_tuples.sort()\n",
    "\n",
    "print(sample_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use exactly the same hyperparameters that were used during training\n",
    "device = inf.device\n",
    "epochs = 15\n",
    "optimizer = 'SGD'\n",
    "lr = 0.01\n",
    "lr_reduce_patience = 5\n",
    "normalization = True\n",
    "model_params_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data in the same way as during training\n",
    "data_loaders = [inf.get_loaders_cifar100_superclass_subsets_pytorch(i,j,128,6,normalization) for (i,j) in sample_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(models, data_loaders, save_path, save_final_params=True):\n",
    "    # returns mean min max of the model runs as formatted in tracked params\n",
    "    tracked_params = []\n",
    "    for model in models:\n",
    "        model = model.to(device)\n",
    "        for loaders in data_loaders: \n",
    "            model, params = inf.train(model, loaders, lr=lr, epochs=epochs, tracking_freq=1,\n",
    "                                      early_stopping_min_epochs=100, device=device, optimizer=optimizer,\n",
    "                                      momentum=0.9, reduce_patience=lr_reduce_patience)\n",
    "            tracked_params.append(params)\n",
    "    # get mean min and max of the tracked params.\n",
    "    p_mean, p_min, p_max = inf.list_tracked_params_to_avg(tracked_params,also_min_max=True)\n",
    "    # save to disk\n",
    "    if save_final_params:\n",
    "        np.save(save_path + '_mean.npy',p_mean)\n",
    "        np.save(save_path+ \"_min.npy\",p_min)\n",
    "        np.save(save_path+ \"_max.npy\",p_max)\n",
    "        \n",
    "    return p_mean, p_min, p_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on num_models_per_experiment randomly initialized models\n",
    "random_custom_models = []\n",
    "for i in range(num_models_per_experiment):\n",
    "    model = resnet18(weights=None).to(device)\n",
    "    model.fc = nn.Linear(512, 10).to(device)\n",
    "    random_custom_models.append(model)\n",
    "    \n",
    "path = \"./experiment_results/tracked_params/random_model\"\n",
    "r_params = eval_models(random_custom_models, data_loaders, path, True)\n",
    "\n",
    "\n",
    "model_params_dict['random initialized'] = r_params\n",
    "\n",
    "plot_path = \"./experiment_results/random_init\"\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=True, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we generate all possible tuples of classes that we used to train on\n",
    "possible_tuples_pretrained = []\n",
    "for i in range(10):\n",
    "    for j in range(i+1,10):\n",
    "        possible_tuples_pretrained.append((i,j))\n",
    "        \n",
    "# randomly sample num_models_per_experiment from the list of possible trained tuples\n",
    "tuples_to_load = random.sample(possible_tuples_pretrained, num_models_per_experiment)\n",
    "print(tuples_to_load)\n",
    "\n",
    "pre_trained_models_subset = []\n",
    "\n",
    "# note that the pretrained models are not contained in the GitHub repository due to their size\n",
    "# to run this experiment, train the models with the same hyperparameters as used in the paper and save them in the folder ./results_training_run2_Adams/models_sgd\n",
    "for (i,j) in tuples_to_load:\n",
    "    model = resnet18(weights=None).to(device)\n",
    "    model.fc = nn.Linear(512,10).to(device)\n",
    "    model.load_state_dict(torch.load(f\"./results_training_run2_Adams/models_sgd/model_{i}_{j}.pt\"))\n",
    "\n",
    "    # we additionally tested with the last layer being randomly initialized, but this did not change the results\n",
    "    # model.fc = nn.Linear(512,10).to(device)\n",
    "    \n",
    "    pre_trained_models_subset.append(model)\n",
    "\n",
    "\n",
    "path = \"./experiment_results/tracked_params/pre_trained_model\"\n",
    "pre_t_params = eval_models(pre_trained_models_subset, data_loaders, path, True)\n",
    "\n",
    "model_params_dict['pre trained'] = pre_t_params\n",
    "\n",
    "plot_path = \"./experiment_results/pretrained_init\"\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing initialization with Gabor Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gabor_filter(size, sigma, theta, Lambda, psi, gamma):\n",
    "    \"\"\"\n",
    "    Generates a Gabor filter with given parameters.\n",
    "    :param size: Size of the filter (size x size).\n",
    "    :param sigma: Standard deviation of the Gaussian envelope.\n",
    "    :param theta: Orientation of the Gabor filter.\n",
    "    :param Lambda: Wavelength of the sinusoidal factor.\n",
    "    :param psi: Phase offset.\n",
    "    :param gamma: Spatial aspect ratio.\n",
    "    :return: Gabor filter as a 2D array.\n",
    "    \"\"\"\n",
    "    sigma_x = sigma\n",
    "    sigma_y = sigma / gamma\n",
    "\n",
    "    # Prepare grid in x and y\n",
    "    x = np.linspace(-size // 2, size // 2, size)\n",
    "    y = np.linspace(-size // 2, size // 2, size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    # Rotation\n",
    "    x_theta = x * np.cos(theta) + y * np.sin(theta)\n",
    "    y_theta = -x * np.sin(theta) + y * np.cos(theta)\n",
    "\n",
    "    gb = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.cos(2 * np.pi / Lambda * x_theta + psi)\n",
    "    return gb\n",
    "\n",
    "def init_gabor_filters(module, random_seed):\n",
    "    # Check if the module is a convolutional layer\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        out_channels = module.out_channels\n",
    "        for i in range(out_channels):\n",
    "            np.random.seed(random_seed)\n",
    "            random_seed += 1\n",
    "\n",
    "            sigma = np.random.uniform(1.5, 2.5)\n",
    "            theta = np.random.uniform(0, np.pi)\n",
    "            Lambda = np.random.uniform(2, 13)\n",
    "            psi = np.random.uniform(0, 2*np.pi)\n",
    "            gamma = np.random.uniform(0.9, 1.1)\n",
    "\n",
    "            for j in range(module.in_channels):\n",
    "                filter_size = module.kernel_size[0] if type(module.kernel_size) is tuple else module.kernel_size\n",
    "                gabor_filter = generate_gabor_filter(filter_size, sigma, theta, Lambda, psi, gamma)\n",
    "                module.weight.data[i, j, :, :] = torch.from_numpy(gabor_filter)\n",
    "                \n",
    "    return random_seed\n",
    "\n",
    "def initialize_model_gabor(num_layers):\n",
    "    model = resnet18(num_classes=10).to(device)\n",
    "    \n",
    "    random_seed = 0\n",
    "    num_initialized = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if 'conv' in name and num_initialized < num_layers:  # This ensures we are only initializing conv layers\n",
    "            random_seed = init_gabor_filters(module, random_seed)\n",
    "            num_initialized += 1\n",
    "\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary this parameter to test different numbers of layers initialized with Gabor filters\n",
    "num_layers = 17\n",
    "\n",
    "# Testing on two random initialized models\n",
    "gabor_models = []\n",
    "for i in range(num_models_per_experiment):\n",
    "    model = initialize_model_gabor(num_layers).to(device)\n",
    "    gabor_models.append(model)\n",
    "    \n",
    "path = f\"./experiment_results/tracked_params/gabor/gabor_model_num_layers_{num_layers}\"\n",
    "gabor_params = eval_models(gabor_models, data_loaders, path, True)\n",
    "\n",
    "model_params_dict[f'gabor initialized {num_layers} layers'] = gabor_params\n",
    "\n",
    "plot_path = f\"./experiment_results/gabor_init_{num_layers}_layers\"\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=True, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for fine filter clustering (Euclid & Fourier)\n",
    "\n",
    "In this approach a 3d filter is separated into 2d filters separated based on the input channels. These 2d filters are then clustered using KMeans for every layer separately, completely ignoring the input channels.\n",
    "\n",
    "## Euclid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load all the models to memory that were pretrained\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "pre_trained_models = []\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i+1,10):\n",
    "        # first check if this file exists: \n",
    "        try:\n",
    "            model = resnet18(weights=None).to(device)\n",
    "            model.fc = nn.Linear(512,10).to(device)\n",
    "            model.load_state_dict(torch.load(f\"./results_training_run2_Adams/models_sgd/model_{i}_{j}.pt\"))\n",
    "            pre_trained_models.append(model)\n",
    "        except:\n",
    "            continue\n",
    "# shuffle the models \n",
    "np.random.shuffle(pre_trained_models)\n",
    "\n",
    "print(\"Loaded\",len(pre_trained_models),\"pre trained models\")\n",
    "\n",
    "def models_to_filter_per_layer(models):\n",
    "    print(\"Using\",len(models),\"models to create filters\")\n",
    "    filters_per_layer = {}\n",
    "    \n",
    "    filters_per_layer[\"0\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"0\"].append(model.conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"1\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"1\"].append(model.layer1[0].conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"2\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"2\"].append(model.layer1[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"3\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"3\"].append(model.layer1[1].conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"4\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"4\"].append(model.layer1[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"5\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"5\"].append(model.layer2[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"6\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"6\"].append(model.layer2[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"7\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"7\"].append(model.layer2[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"8\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"8\"].append(model.layer2[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"9\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"9\"].append(model.layer3[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"10\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"10\"].append(model.layer3[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"11\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"11\"].append(model.layer3[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"12\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"12\"].append(model.layer3[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"13\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"13\"].append(model.layer4[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"14\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"14\"].append(model.layer4[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"15\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"15\"].append(model.layer4[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"16\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"16\"].append(model.layer4[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    for key in filters_per_layer.keys():\n",
    "        filters_per_layer[key] = np.array(filters_per_layer[key])\n",
    "\n",
    "    return filters_per_layer\n",
    "\n",
    "\n",
    "def clustering_for_single_layer(filters_per_layer,layer_key, num_clusters):\n",
    "    filters_per_layer[layer_key] = np.array(filters_per_layer[layer_key])\n",
    "    # reshape every single filter to a 1d array\n",
    "    filters_per_layer[layer_key] = filters_per_layer[layer_key].reshape(-1, *filters_per_layer[layer_key].shape[3:])\n",
    "    filters_per_layer[layer_key] = filters_per_layer[layer_key].reshape(filters_per_layer[layer_key].shape[0],-1)\n",
    "\n",
    "    # now it has the form #filters x #elements in filter (where 1 filter is one 2d array, stored as 1d)\n",
    "    kmeans_filters = KMeans(n_clusters=num_clusters, n_init='auto',)\n",
    "    cluster_labels = kmeans_filters.fit_predict(filters_per_layer[layer_key])\n",
    "    \n",
    "    # cluster centers:\n",
    "    cluster_centers = kmeans_filters.cluster_centers_\n",
    "    return cluster_labels, cluster_centers\n",
    "\n",
    "def clustering_single_kmeans(models,device, num_clusters=30, num_layers=17):\n",
    "    # num layers need to be at least 1 and at most 17\n",
    "    assert num_layers >= 1 and num_layers <= 17\n",
    "\n",
    "    \n",
    "    # get dict of filters for every layer\n",
    "    filters_per_layer = models_to_filter_per_layer(models)\n",
    "    keys = list(filters_per_layer.keys())\n",
    "\n",
    "    new_filters_per_layer = {}\n",
    "\n",
    "    for layer_key in keys:\n",
    "\n",
    "        orig_shape = filters_per_layer[layer_key].shape\n",
    "        labels, centers = clustering_for_single_layer(filters_per_layer,layer_key,num_clusters)\n",
    "        # create a prob density vector of the labels\n",
    "        pdf = np.zeros(num_clusters)\n",
    "        for i in range(labels.shape[0]):\n",
    "            pdf[labels[i]] += 1\n",
    "        pdf = pdf/labels.shape[0]\n",
    "        # construct the vector of # filters x centroids \n",
    "        new_filters = np.zeros(orig_shape[1:])\n",
    "        #print(\"new filters\",new_filters.shape)    \n",
    "\n",
    "        for i in range(new_filters.shape[0]):\n",
    "            for j in range(new_filters.shape[1]):\n",
    "                # sample an index from this pdf, this is the index of the cluster that we will use\n",
    "                sampled_index = np.random.choice(num_clusters, p=pdf)\n",
    "                #print(sampled_index)\n",
    "                new_filters[i][j] = centers[sampled_index].reshape((new_filters.shape[2],new_filters.shape[3]))\n",
    "\n",
    "        new_filters_per_layer[layer_key] = new_filters\n",
    "\n",
    "     \n",
    "        \n",
    "    model = resnet18(num_classes=10).to(device)\n",
    "    \n",
    "    # go over all layers and set the new filters\n",
    "    model.conv1.weight.data = torch.tensor(new_filters_per_layer[\"0\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 1: return model\n",
    "    model.layer1[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"1\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 2: return model\n",
    "    model.layer1[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"2\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 3: return model\n",
    "    model.layer1[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"3\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 4: return model\n",
    "    model.layer1[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"4\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 5: return model\n",
    "    model.layer2[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"5\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 6: return model\n",
    "    model.layer2[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"6\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 7: return model\n",
    "    model.layer2[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"7\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 8: return model\n",
    "    model.layer2[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"8\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 9: return model\n",
    "    model.layer3[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"9\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 10: return model\n",
    "    model.layer3[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"10\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 11: return model\n",
    "    model.layer3[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"11\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 12: return model\n",
    "    model.layer3[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"12\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 13: return model\n",
    "    model.layer4[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"13\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 14: return model\n",
    "    model.layer4[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"14\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 15: return model\n",
    "    model.layer4[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"15\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 16: return model\n",
    "    model.layer4[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"16\"], dtype=torch.float32).to(device)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def dft(weights):\n",
    "    ffts = []\n",
    "    for i in range(weights.shape[0]):\n",
    "        ffts.append(np.fft.fft2(weights[i]))\n",
    "    ffts = np.array(ffts)\n",
    "    return np.real(ffts), np.imag(ffts)\n",
    "\n",
    "def inverse_dft(cluster_results):\n",
    "    weights = []\n",
    "    for i in range(cluster_results.shape[0]):\n",
    "        weights.append(np.fft.ifft2(cluster_results[i]))\n",
    "    weights = np.array(weights)\n",
    "    return np.real(weights), np.imag(weights)\n",
    "\n",
    "def clustering_fourier_single_layer(filters, num_clusters):\n",
    "    filters_reshaped = np.reshape(filters, (filters.shape[0]*filters.shape[1]*filters.shape[2],filters.shape[3],filters.shape[4]))\n",
    "    dfts, _ = dft(filters_reshaped)\n",
    "    dfts_reshaped = np.reshape(dfts,(dfts.shape[0],dfts.shape[1]*dfts.shape[2]))\n",
    "    kmeans = KMeans(n_clusters = num_clusters, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(dfts_reshaped)\n",
    "    \n",
    "    final_weights, _ = inverse_dft(np.reshape(kmeans.cluster_centers_, (kmeans.cluster_centers_.shape[0],dfts.shape[1],dfts.shape[2])))\n",
    "    return cluster_labels, final_weights\n",
    "\n",
    "def clustering_single_fourier(models,device, num_clusters=30, num_layers=17):\n",
    "    # num layers need to be at least 1 and at most 17\n",
    "    assert num_layers >= 1 and num_layers <= 17\n",
    "\n",
    "    \n",
    "    # get dict of filters for every layer\n",
    "    filters_per_layer = models_to_filter_per_layer(models)\n",
    "    keys = list(filters_per_layer.keys())\n",
    "\n",
    "    new_filters_per_layer = {}\n",
    "\n",
    "    \n",
    "    for layer_key in keys:\n",
    "\n",
    "        orig_shape = filters_per_layer[layer_key].shape\n",
    "        \n",
    "        labels, centers = clustering_fourier_single_layer(filters_per_layer[layer_key], num_clusters)\n",
    "                \n",
    "        # create a prob density vector of the labels\n",
    "        pdf = np.zeros(num_clusters)\n",
    "        for i in range(labels.shape[0]):\n",
    "            pdf[labels[i]] += 1\n",
    "        pdf = pdf/labels.shape[0]\n",
    "        # construct the vector of # filters x centroids \n",
    "        new_filters = np.zeros(orig_shape[1:])\n",
    "\n",
    "        for i in range(new_filters.shape[0]):\n",
    "            for j in range(new_filters.shape[1]):\n",
    "                # sample an index from this pdf, this is the index of the cluster that we will use\n",
    "                sampled_index = np.random.choice(num_clusters, p=pdf)\n",
    "                #print(sampled_index)\n",
    "                new_filters[i][j] = centers[sampled_index].reshape((new_filters.shape[2],new_filters.shape[3]))\n",
    "\n",
    "        new_filters_per_layer[layer_key] = new_filters\n",
    "\n",
    "     \n",
    "        \n",
    "    model = resnet18(num_classes=10).to(device)\n",
    "    \n",
    "    # go over all layers and set the new filters\n",
    "    model.conv1.weight.data = torch.tensor(new_filters_per_layer[\"0\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 1: return model\n",
    "    model.layer1[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"1\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 2: return model\n",
    "    model.layer1[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"2\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 3: return model\n",
    "    model.layer1[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"3\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 4: return model\n",
    "    model.layer1[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"4\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 5: return model\n",
    "    model.layer2[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"5\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 6: return model\n",
    "    model.layer2[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"6\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 7: return model\n",
    "    model.layer2[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"7\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 8: return model\n",
    "    model.layer2[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"8\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 9: return model\n",
    "    model.layer3[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"9\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 10: return model\n",
    "    model.layer3[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"10\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 11: return model\n",
    "    model.layer3[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"11\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 12: return model\n",
    "    model.layer3[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"12\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 13: return model\n",
    "    model.layer4[0].conv1.weight.data = torch.tensor(new_filters_per_layer[\"13\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 14: return model\n",
    "    model.layer4[0].conv2.weight.data = torch.tensor(new_filters_per_layer[\"14\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 15: return model\n",
    "    model.layer4[1].conv1.weight.data = torch.tensor(new_filters_per_layer[\"15\"], dtype=torch.float32).to(device)\n",
    "    if num_layers == 16: return model\n",
    "    model.layer4[1].conv2.weight.data = torch.tensor(new_filters_per_layer[\"16\"], dtype=torch.float32).to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the number of pretrained models used for clustering\n",
    "\n",
    "For this experiment the number of clusters is set 10 and the number of layers is set to all 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_num_models_used = [2,4,6,10,20]\n",
    "\n",
    "num_models_dict = {}\n",
    "\n",
    "for num_models_used in choices_num_models_used:\n",
    "    # create a subset of the pre trained models\n",
    "    subset_models = pre_trained_models[:num_models_used]\n",
    "    \n",
    "    # create a model with the clustered filters\n",
    "    clustered_models_euclid = [clustering_single_kmeans(subset_models, device, num_clusters=10, num_layers=17) for i in range(num_models_per_experiment)]\n",
    "    clustered_models_fourier = [clustering_single_fourier(subset_models, device, num_clusters=10, num_layers=17) for i in range(num_models_per_experiment)]\n",
    "    \n",
    "    # evaluate the model\n",
    "    path = f\"./experiment_results/tracked_params/euclid/num_models/euclid_model_num_models_{num_models_used}\"\n",
    "    euclid_params = eval_models(clustered_models_euclid, data_loaders, path, True)\n",
    "    path = f\"./experiment_results/tracked_params/fourier/num_models/fourier_model_num_models_{num_models_used}\"\n",
    "    fourier_params = eval_models(clustered_models_fourier, data_loaders, path, True)\n",
    "    \n",
    "    model_params_dict[f'euclid #models{num_models_used}'] = euclid_params\n",
    "    model_params_dict[f'fourier #models{num_models_used}'] = fourier_params\n",
    "    \n",
    "    num_models_dict[f'euclid #models{num_models_used}'] = euclid_params\n",
    "    num_models_dict[f'fourier #models{num_models_used}'] = fourier_params\n",
    "\n",
    "plot_path = \"./experiment_results/clustered_num_models\"\n",
    "inf.plot_trainings_mean_min_max(num_models_dict,display_train_acc=False,display_only_mean=True,save=True,save_path=plot_path,display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the number of clusters used for clustering\n",
    "\n",
    "For this experiment the number of pretrained models used is set to 10 and the number of layers is set to all 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_num_clusters = [1,2,3,10,30,50] \n",
    "\n",
    "num_clusters_dict = {}\n",
    "\n",
    "for num_clusters in choices_num_clusters:\n",
    "    # create a subset of the pre trained models\n",
    "    subset_models = pre_trained_models[:10]\n",
    "    \n",
    "    # create a model with the clustered filters\n",
    "    euclid_models = [clustering_single_kmeans(subset_models, device, num_clusters=num_clusters, num_layers=17) for i in range(num_models_per_experiment)]\n",
    "    fourier_models = [clustering_single_fourier(subset_models, device, num_clusters=num_clusters, num_layers=17) for i in range(num_models_per_experiment)]\n",
    "\n",
    "    # evaluate the model\n",
    "    path = f\"./experiment_results/tracked_params/euclid/num_clusters/euclid_model_num_clusters_{num_clusters}\"\n",
    "    euclid_params = eval_models(euclid_models, data_loaders, path, True)\n",
    "    path = f\"./experiment_results/tracked_params/fourier/num_clusters/fourier_model_num_clusters_{num_clusters}\"\n",
    "    fourier_params = eval_models(fourier_models, data_loaders, path, True)\n",
    "    \n",
    "    \n",
    "    model_params_dict[f'euclid #clusters{num_clusters}'] = euclid_params\n",
    "    model_params_dict[f'fourier #clusters{num_clusters}'] = fourier_params\n",
    "    \n",
    "    num_clusters_dict[f'euclid #clusters{num_clusters}'] = euclid_params\n",
    "    num_clusters_dict[f'fourier #clusters{num_clusters}'] = fourier_params\n",
    "\n",
    "plot_path = \"./experiment_results/clustered_num_clusters\"\n",
    "inf.plot_trainings_mean_min_max(num_clusters_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the number of layers used for clustering\n",
    "\n",
    "For this experiment the number of pretrained models used is set to 10 and the number of clusters is set to all 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_num_layers = [1,2,6,10,17]\n",
    "num_filters_dict = {}\n",
    "\n",
    "for num_layers in choices_num_layers:\n",
    "    # create a subset of the pre trained models\n",
    "    subset_models = pre_trained_models[:10]\n",
    "\n",
    "    # create a model with the clustered filters\n",
    "    euclid_models = [clustering_single_kmeans(subset_models, device, num_clusters=10, num_layers=num_layers) for i in range(num_models_per_experiment)]\n",
    "    fourier_models = [clustering_single_fourier(subset_models, device, num_clusters=10, num_layers=num_layers) for i in range(num_models_per_experiment)]\n",
    "\n",
    "    # evaluate the model\n",
    "    path = f\"./experiment_results/tracked_params/euclid/num_layers/euclid_model_num_layers_{num_layers}\"\n",
    "    euclid_params = eval_models(euclid_models, data_loaders, path, True)\n",
    "    path = f\"./experiment_results/tracked_params/fourier/num_layers/fourier_model_num_layers_{num_layers}\"\n",
    "    fourier_params = eval_models(fourier_models, data_loaders, path, True)\n",
    "    \n",
    "    model_params_dict[f'euclid #layers{num_layers}'] = euclid_params\n",
    "    model_params_dict[f'fourier #layers{num_layers}'] = fourier_params\n",
    "    \n",
    "    num_filters_dict[f'euclid #layers{num_layers}'] = euclid_params\n",
    "    num_filters_dict[f'fourier #layers{num_layers}'] = fourier_params\n",
    "\n",
    "plot_path = \"./experiment_results/clustered_num_layers\"\n",
    "inf.plot_trainings_mean_min_max(num_filters_dict,display_train_acc=False,display_only_mean=True,save=True,save_path=plot_path,display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the alignment algorithm\n",
    "\n",
    "## Randomly initialized custom ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_ResNet18 import custom_resnet_18\n",
    "\n",
    "random_custom_models = []\n",
    "for i in range(num_models_per_experiment):\n",
    "    model = custom_resnet_18(num_classes=10).to(device)\n",
    "    random_custom_models.append(model)\n",
    "    \n",
    "path = \"./experiment_results/tracked_params/alignment/custom_random_model\"\n",
    "r_params = eval_models(random_custom_models, data_loaders, path, True)\n",
    "\n",
    "model_params_dict['random initialized'] = r_params\n",
    "\n",
    "plot_path = \"./experiment_results/custom_random_init\"\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=True, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered custom ResNet-18 with alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_custom_pretrained = \"./\"\n",
    "\n",
    "def load_custom_pre_trained_models():\n",
    "\n",
    "    pre_trained_models = []\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(i+1,10):\n",
    "            # first check if this file exists: \n",
    "            try:\n",
    "                model = custom_resnet_18(num_classes=10).to(device)\n",
    "                model.load_state_dict(torch.load(f'{path_to_custom_pretrained}model_{i}_{j}.pt'))\n",
    "                pre_trained_models.append(model)\n",
    "            except:\n",
    "                print(f\"Pretrained model {(i,j)} not found\")\n",
    "                continue\n",
    "\n",
    "    # randomly shuffle the models\n",
    "    np.random.shuffle(pre_trained_models)\n",
    "\n",
    "    return pre_trained_models\n",
    "\n",
    "def dft(weights):\n",
    "    return np.real(np.fft.fft(weights))\n",
    "\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    # Euclidean distance is the square root of the sum of the squared differences\n",
    "    return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
    "\n",
    "def calculate_distance(cluster_group, vec):\n",
    "    total_distance = 0\n",
    "\n",
    "    fourier_vec = dft(vec)\n",
    "    for cluster_vec in cluster_group:\n",
    "        \n",
    "        fourier_cluster_vec = dft(cluster_vec)\n",
    "\n",
    "        distance = euclidean_distance(fourier_vec, fourier_cluster_vec)\n",
    "        total_distance += distance\n",
    "\n",
    "    return total_distance\n",
    "\n",
    "def get_custom_clustering_groups(filters_per_model, num_filters_per_model):\n",
    "    filter_vectors = []\n",
    "    \n",
    "    num_models = len(filters_per_model)\n",
    "    \n",
    "    for idx, filters in enumerate(filters_per_model):\n",
    "        \n",
    "        filter_vectors.append([])\n",
    "\n",
    "        for i in range(filters.shape[0]):\n",
    "            # Flatten and normalize the filter\n",
    "            filter_vec = filters[i].flatten()\n",
    "            filter_vec /= np.linalg.norm(filter_vec)\n",
    "            filter_vectors[idx].append(filter_vec)\n",
    "            \n",
    "    new_groups = [[] for _ in range(num_filters_per_model)]\n",
    "    permutations = [[] for _ in range(num_models)]\n",
    "    \n",
    "    # Now we go through all models and assign filters in a greedy manner\n",
    "    for i in range(num_models):\n",
    "        model_filters = filter_vectors[i]\n",
    "        used_indices = set()\n",
    "        \n",
    "        # We go through every group of the new clusters and assign the min distance vector from the next model to it\n",
    "        for cluster_idx, cluster_group in enumerate(new_groups):\n",
    "            min_distance = float('inf')\n",
    "            selected_vector_idx = None\n",
    "            \n",
    "            for idx, vec in enumerate(model_filters):\n",
    "                dist = calculate_distance(cluster_group, vec)\n",
    "                if dist < min_distance and idx not in used_indices:\n",
    "                    min_distance = dist\n",
    "                    selected_vector_idx = idx\n",
    "            \n",
    "            new_groups[cluster_idx].append(model_filters[selected_vector_idx])\n",
    "            used_indices.add(selected_vector_idx)\n",
    "            permutations[i].append(selected_vector_idx)\n",
    "\n",
    "    return new_groups, permutations\n",
    "\n",
    "def custom_clustering(filters, num_input_channels, num_filters, kernel_size, include_permutations=False):\n",
    "    \n",
    "    clustered_groups, permutations = get_custom_clustering_groups(filters, num_filters)\n",
    "    \n",
    "    filter_vectors = np.array(clustered_groups)\n",
    "    average_filters = [np.mean(np.array(vectors), axis=0) for vectors in filter_vectors]\n",
    "    \n",
    "    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n",
    "    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n",
    "    \n",
    "    if include_permutations:\n",
    "        return torch_shaped_average_filters, permutations\n",
    "    else:\n",
    "        return torch_shaped_average_filters\n",
    "    \n",
    "def permute_bias(permuted_model_bias, perm):\n",
    "    permuted_model_bias.weight = torch.nn.Parameter(permuted_model_bias.weight[perm])\n",
    "    permuted_model_bias.bias = torch.nn.Parameter(permuted_model_bias.bias[perm])\n",
    "    permuted_model_bias.running_mean = permuted_model_bias.running_mean[perm]\n",
    "    permuted_model_bias.running_var = permuted_model_bias.running_var[perm]\n",
    "\n",
    "def permute_layer(permuted_layer, permuted_layer_next, permuted_bias, perm):\n",
    "    permuted_layer.weight = torch.nn.Parameter(permuted_layer.weight[perm])\n",
    "    permute_bias(permuted_bias, perm)\n",
    "    permuted_layer_next.weight = torch.nn.Parameter(permuted_layer_next.weight.transpose(0,1)[perm].transpose(0,1))\n",
    "\n",
    "def create_initialized_model(models, num_layers):\n",
    "    final_model = custom_resnet_18(num_classes=10).to(device)\n",
    "    \n",
    "    trained_filters_layer1 = [model.conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer1, permutation_layer1 = custom_clustering(filters=trained_filters_layer1,\n",
    "                                                            num_input_channels=3,\n",
    "                                                            num_filters=64,\n",
    "                                                            kernel_size=7,\n",
    "                                                            include_permutations=True)\n",
    "    final_model.conv1.weight.data = sampled_filters_layer1\n",
    "    if num_layers == 1:\n",
    "        return final_model\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        permute_layer(model.conv1, model.layer1[0].conv1, model.bn1, permutation_layer1[idx])\n",
    "    \n",
    "    trained_filters_layer2 = [model.layer1[0].conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer2, permutation_layer2 = custom_clustering(filters=trained_filters_layer2,\n",
    "                                                            num_input_channels=64,\n",
    "                                                            num_filters=64,\n",
    "                                                            kernel_size=3,\n",
    "                                                            include_permutations=True)\n",
    "    \n",
    "    final_model.layer1[0].conv1.weight.data = sampled_filters_layer2\n",
    "    if num_layers == 2:\n",
    "        return final_model\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        permute_layer(model.layer1[0].conv1, model.layer1[0].conv2, model.layer1[0].bn1, permutation_layer2[idx])\n",
    "    \n",
    "    \n",
    "    trained_filters_layer3 = [model.layer1[0].conv2.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer3, permutation_layer3 = custom_clustering(filters=trained_filters_layer3,\n",
    "                                                            num_input_channels=64,\n",
    "                                                            num_filters=64,\n",
    "                                                            kernel_size=3,\n",
    "                                                            include_permutations=True)\n",
    "    final_model.layer1[0].conv2.weight.data = sampled_filters_layer3\n",
    "    if num_layers == 3:\n",
    "        return final_model\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        permute_layer(model.layer1[0].conv2, model.layer1[1].conv1, model.layer1[0].bn2, permutation_layer3[idx])\n",
    "        \n",
    "    trained_filters_layer4 = [model.layer1[1].conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer4, permutation_layer4 = custom_clustering(filters=trained_filters_layer4,\n",
    "                                                            num_input_channels=64,\n",
    "                                                            num_filters=64,\n",
    "                                                            kernel_size=3,\n",
    "                                                            include_permutations=True)\n",
    "    final_model.layer1[1].conv1.weight.data = sampled_filters_layer4\n",
    "    if num_layers == 4:\n",
    "        return final_model\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        permute_layer(model.layer1[1].conv1, model.layer1[1].conv2, model.layer1[1].bn1, permutation_layer4[idx])\n",
    "\n",
    "    trained_filters_layer5 = [model.layer1[1].conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer5, permutation_layer5 = custom_clustering(filters=trained_filters_layer4,\n",
    "                                                            num_input_channels=64,\n",
    "                                                            num_filters=64,\n",
    "                                                            kernel_size=3,\n",
    "                                                            include_permutations=True)\n",
    "    final_model.layer1[1].conv1.weight.data = sampled_filters_layer4\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_pre_trained_models = load_custom_pre_trained_models()\n",
    "num_models_dict = {}\n",
    "num_models_used = 6\n",
    "num_layers_candidates = [1,2,4]\n",
    "\n",
    "subset_models = custom_pre_trained_models[:num_models_used]\n",
    "\n",
    "for num_layers in num_layers_candidates:\n",
    "\tclustered_models_alignment = []\n",
    "\n",
    "\tfor i in range(num_models_per_experiment):\n",
    "\t\tclustered_models_alignment.append(create_initialized_model(subset_models, num_layers))\n",
    "\n",
    "\tpath = f\"./experiment_results/tracked_params/alignment/custom_alignment_num_layers_{num_layers}\"\n",
    "\talignment_params = eval_models(clustered_models_alignment, data_loaders, path, True)\n",
    "\n",
    "\tnum_models_dict[f'Alignment {num_layers} layers'] = alignment_params\n",
    "\n",
    "plot_path = \"./experiment_results/clustered_num_models\"\n",
    "inf.plot_trainings_mean_min_max(num_models_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Tiny ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CustomDataset(torch.utils.data.dataset.Dataset):\n",
    "    \n",
    "    def __init__(self, path, is_train=True):\n",
    "        \n",
    "\t\t# due to the size of the dataset, it was not included in the GitHub repository\n",
    "        # the dataset can be downloaded from https://www.kaggle.com/datasets/akash2sharma/tiny-imagenet\n",
    "        with open(f'{path}/wnids.txt') as f:\n",
    "            self.labels = [x.strip() for x in f.readlines()]\n",
    "            \n",
    "        if not is_train:\n",
    "            # Load the labels and file names from val_annotations.txt for the test set\n",
    "            self.labels_map = {}\n",
    "            with open(os.path.join(path, 'val_annotations.txt')) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    self.labels_map[parts[0]] = parts[1]  # map filename to label\n",
    "\n",
    "            path = os.path.join(path, 'images')  # Update path to images directory\n",
    "\n",
    "        self.files = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                if f.endswith('JPEG'):\n",
    "                    fullpath = os.path.join(root, f)\n",
    "                    self.files.append(fullpath)\n",
    "                    \n",
    "        TIN_MEAN = [0.485, 0.456, 0.406]\n",
    "        TIN_STD = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        self.transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(TIN_MEAN, TIN_STD)\n",
    "        ])\n",
    "        self.is_train = is_train\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    "        img = Image.open(file).convert('RGB')\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        if self.is_train:\n",
    "            label_name = file.split('/')[-1].split('_')[0]\n",
    "            label = self.labels.index(label_name)\n",
    "        else:\n",
    "            file_name = file.split('/')[-1]\n",
    "            label_name = self.labels_map[file_name]\n",
    "            label = self.labels.index(label_name)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "# Training set\n",
    "trainset = CustomDataset('./tiny_imagenet/train', is_train=True)\n",
    "# Test set\n",
    "testset = CustomDataset('./tiny_imagenet/val', is_train=False)\n",
    "\n",
    "tinyIN_loaders = [{\n",
    "    'train': torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4),\n",
    "    'test': torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=4),\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_dict = {}\n",
    "\n",
    "# Testing on two random initialized models\n",
    "tiny_random_model = []\n",
    "for i in range(num_models_per_experiment):\n",
    "    model = resnet18(num_classes=200).to(device)\n",
    "    tiny_random_model.append(model)\n",
    "    \n",
    "path = \"./experiment_results/tracked_params/imagenet/tiny_imagenet_random_models\"\n",
    "r_params = eval_models(tiny_random_model, tinyIN_loaders, path, True)\n",
    "\n",
    "model_params_dict['ImageNet 5 random initialized models'] = r_params\n",
    "\n",
    "plot_path = \"./experiment_results/random_IN_init\"\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_models_subset = pre_trained_models[0:5]\n",
    "\n",
    "path = f'./experiment_results/tracked_params/imagenet/tiny_imagenet_pretrained_models'\n",
    "pretrained_params = eval_models(pre_trained_models_subset, tinyIN_loaders, path, True)\n",
    "\n",
    "model_params_dict[f'ImageNet 5 pretrained models'] = pretrained_params\n",
    "\n",
    "plot_path = f'./experiment_results/pretrained_IN_init'\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models_used = 6\n",
    "num_clusters = 3\n",
    "num_layers = 17\n",
    "\n",
    "clustered_models = []\n",
    "for i in range(num_models_per_experiment):  # If you want more models, change the range accordingly\n",
    "    # Calculate the start and end indices for the slice of models to be used\n",
    "    start_idx = i * num_models_used\n",
    "    end_idx = start_idx + num_models_used\n",
    "\n",
    "    # Slice the pre-trained models accordingly\n",
    "    subset_models = pre_trained_models[start_idx:end_idx]\n",
    "\n",
    "    # Continue with clustering and saving the model\n",
    "    model = clustering_single_fourier(subset_models, device, num_clusters=num_clusters, num_layers=num_layers)\n",
    "    clustered_models.append(model)\n",
    "    print(f\"Finished clustering [{i+1}/{num_models_per_experiment}] models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'./experiment_results/tracked_params/imagenet/tiny_imagenet_clustered_models'\n",
    "clustered_params = eval_models(clustered_models, tinyIN_loaders, path, True)\n",
    "\n",
    "model_params_dict[f'ImageNet 5 clustered models'] = clustered_params\n",
    "\n",
    "plot_path = f'./experiment_results/clustered_IN_init'\n",
    "inf.plot_trainings_mean_min_max(model_params_dict, display_train_acc=False, display_only_mean=True, save=True, save_path=plot_path, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
