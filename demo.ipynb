{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ResNet18 & CIFAR-100\n",
    "\n",
    "Demo File to test fast training one ResNet18 Model takes on CIFAR100, on a subset of 10 classes.\n",
    "\n",
    "Source: Most of this code is taken from FFCV CIFAR10 Example!\n",
    "https://docs.ffcv.io/ffcv_examples/cifar10.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and storing CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as ch\n",
    "import torch    \n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "\n",
    "from fastargs import get_current_config, Param, Section\n",
    "from fastargs.decorators import param\n",
    "from fastargs.validation import And, OneOf\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that loads the cifar100 datasets and stores it as a .beton (ffcv) file\n",
    "\n",
    "\n",
    "Source: https://docs.ffcv.io/writing_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cifar100(train_dataset=\"./data/cifar_train.beton\", val_dataset=\"./data/cifar_test.beton\"):\n",
    "    datasets = {\n",
    "        'train': torchvision.datasets.CIFAR100('./data', train=True, download=True),\n",
    "        'test': torchvision.datasets.CIFAR100('./data', train=False, download=True)\n",
    "        }\n",
    "\n",
    "    for (name, ds) in datasets.items():\n",
    "        path = train_dataset if name == 'train' else val_dataset\n",
    "        writer = DatasetWriter(path, {\n",
    "            'image': RGBImageField(),\n",
    "            'label': IntField()\n",
    "        })\n",
    "        writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 122966.22it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 33110.80it/s]\n"
     ]
    }
   ],
   "source": [
    "load_cifar100()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as ch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss, Conv2d, BatchNorm2d\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torchvision\n",
    "\n",
    "from fastargs import get_current_config, Param, Section\n",
    "from fastargs.decorators import param\n",
    "from fastargs.validation import And, OneOf\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.pipeline.operation import Operation\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "from ffcv.writer import DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(train_dataset=\"./data/cifar_train.beton\", val_dataset=\"./data/cifar_test.beton\", batch_size=256, num_workers=12):\n",
    "    paths = {\n",
    "        'train': train_dataset,\n",
    "        'test': val_dataset\n",
    "\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    # https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151#file-cifar100_mean_std-py\n",
    "    # this source details how and what mean and std of the datasets are\n",
    "    # took the values from the source above and multiplied by 255\n",
    "    # not sure this properly translates to std dev of the dataset TODO: check this\t\n",
    "    CIFAR_MEAN = [129.310, 124.108, 112.404]\n",
    "    CIFAR_STD = [68.2125, 65.4075, 70.4055]\n",
    "    loaders = {}\n",
    "\n",
    "    for name in ['train', 'test']:\n",
    "        label_pipeline: List[Operation] = [IntDecoder(), ToTensor(), ToDevice(ch.device(device)), Squeeze()]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "        if name == 'train':\n",
    "            image_pipeline.extend([\n",
    "                RandomHorizontalFlip(),\n",
    "                RandomTranslate(padding=2, fill=tuple(map(int, CIFAR_MEAN))),\n",
    "                Cutout(4, tuple(map(int, CIFAR_MEAN))),\n",
    "            ])\n",
    "        image_pipeline.extend([\n",
    "            ToTensor(),\n",
    "            ToDevice(ch.device(device), non_blocking=True),\n",
    "            ToTorchImage(),\n",
    "            Convert(ch.float32), # TODO check what the impact for float16 is (it was the initial value, and why it crashes with float16)\t\n",
    "            torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "        ])\n",
    "        \n",
    "        ordering = OrderOption.RANDOM if name == 'train' else OrderOption.SEQUENTIAL\n",
    "\n",
    "        loaders[name] = Loader(paths[name], batch_size=batch_size, num_workers=num_workers,\n",
    "                               order=ordering, drop_last=(name == 'train'),\n",
    "                               pipelines={'image': image_pipeline, 'label': label_pipeline})\n",
    "\n",
    "    return loaders, start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(output_dim:int = 100):\n",
    "    \n",
    "    #ResNet general source: https://pytorch.org/vision/master/models/resnet.html\n",
    "    \n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    # make fc a sequential layer\n",
    "    model.fc = ch.nn.Sequential(ch.nn.Linear(model.fc.in_features, output_dim), ch.nn.Softmax(dim=1))\n",
    "    model = model.to(device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, lr=0.1, epochs=50, momentum=0.9, weight_decay=0.0001, lr_peak_epoch=5):\n",
    "    \n",
    "    opt = SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    iters_per_epoch = len(loaders['train'])\n",
    "    # Cyclic LR with single triangle\n",
    "    lr_schedule = np.interp(np.arange((epochs+1) * iters_per_epoch),\n",
    "                            [0, lr_peak_epoch * iters_per_epoch, epochs * iters_per_epoch],\n",
    "                            [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(opt, lr_schedule.__getitem__)\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for ims, labs in tqdm(loaders['train']):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(ims)\n",
    "                loss = loss_fn(out, labs)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loaders, lr_tta=False):\n",
    "    # lr_tta: whether to use test-time augmentation by flipping images horizontally\n",
    "    model.eval()\n",
    "    with ch.no_grad():\n",
    "        for name in ['train', 'test']:\n",
    "            total_correct, total_num = 0., 0.\n",
    "            for ims, labs in tqdm(loaders[name]):\n",
    "                with autocast():\n",
    "                    out = model(ims)\n",
    "                    if lr_tta:\n",
    "                        out += model(ims.flip(-1))\n",
    "                    total_correct += out.argmax(1).eq(labs).sum().cpu().item()\n",
    "                    total_num += ims.shape[0]\n",
    "            print(f'{name} accuracy: {total_correct / total_num * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/janulm/miniconda3/envs/ffcv_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 195/195 [00:13<00:00, 14.66it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 68.88it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.98it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 67.73it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 66.83it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 65.25it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 71.55it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 71.03it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 96.30it/s] \n",
      "100%|██████████| 195/195 [00:02<00:00, 68.21it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 68.91it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 90.12it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 97.10it/s] \n",
      "100%|██████████| 195/195 [00:02<00:00, 68.95it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.97it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 74.84it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 92.67it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 76.57it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.81it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 60.68it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 77.87it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 69.89it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 63.32it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 63.12it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 90.36it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.22it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 69.67it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 71.16it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 75.67it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 69.62it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 68.53it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.68it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 77.65it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 87.29it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 74.11it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.41it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 68.10it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 70.76it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 67.32it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.75it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 69.54it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 70.67it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 79.15it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 74.35it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 84.45it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 64.99it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 64.93it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 62.93it/s]\n",
      "100%|██████████| 195/195 [00:03<00:00, 64.40it/s]\n",
      "100%|██████████| 195/195 [00:02<00:00, 72.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 146.40217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [00:00<00:00, 199.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 20.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 56.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 17.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaders, start_time = make_dataloaders()\n",
    "model = generate_model()\n",
    "train(model, loaders)\n",
    "print(f'Total time: {time.time() - start_time:.5f}')\n",
    "evaluate(model, loaders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
