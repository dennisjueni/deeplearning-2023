{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model in PyTorch\n",
    "class MyCNNModel(nn.Module):\n",
    "    def __init__(self, no_classes):\n",
    "        super(MyCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3)) # output shape: \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1024, 256, bias=False)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, no_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def generate_model(output_dim=100):\n",
    "    model = MyCNNModel(output_dim)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_bias(model_bias, permuted_model_bias, perm):\n",
    "    permuted_model_bias.weight = torch.nn.Parameter(model_bias.weight[perm])\n",
    "    permuted_model_bias.bias = torch.nn.Parameter(model_bias.bias[perm])\n",
    "    permuted_model_bias.running_mean = model_bias.running_mean[perm]\n",
    "    permuted_model_bias.running_var = model_bias.running_var[perm]\n",
    "\n",
    "def permute_weights(model, perm1, perm2, perm3) -> torchvision.models.resnet.ResNet:\n",
    "    with torch.no_grad():\n",
    "            permuted_model = copy.deepcopy(model)\n",
    "            permuted_model.conv1.weight = torch.nn.Parameter(model.conv1.weight[perm1])\n",
    "            permuted_model.conv1.bias = torch.nn.Parameter(model.conv1.bias[perm1])\n",
    "            permuted_model.conv2.weight =  torch.nn.Parameter(model.conv2.weight.transpose(0,1)[perm1].transpose(0,1))\n",
    "\n",
    "            permuted_model.conv2.weight = torch.nn.Parameter(permuted_model.conv2.weight[perm2])\n",
    "            permuted_model.conv2.bias = torch.nn.Parameter(model.conv2.bias[perm2])\n",
    "            permuted_model.conv3.weight =  torch.nn.Parameter(model.conv3.weight.transpose(0,1)[perm2].transpose(0,1))\n",
    "\n",
    "            permuted_model.conv3.weight = torch.nn.Parameter(permuted_model.conv3.weight[perm3])\n",
    "            permuted_model.conv3.bias = torch.nn.Parameter(model.conv3.bias[perm3])\n",
    "            permuted_model.fc1.weight =  torch.nn.Parameter(model.fc1.weight[perm3])\n",
    "    return permuted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ldiazbone/Developer/deep-learning-advanced-initialization/alignment_custom.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ldiazbone/Developer/deep-learning-advanced-initialization/alignment_custom.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m permuted_model\u001b[39m.\u001b[39mfc1(model\u001b[39m.\u001b[39mflatten(torch\u001b[39m.\u001b[39mtensor([model_thi_output[\u001b[39m0\u001b[39m][perm3]])))[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0198,  0.0176,  0.0182,  0.0076,  0.0183,  0.0830, -0.0218, -0.0199,\n",
       "        -0.0614,  0.0099,  0.0213,  0.0527,  0.0329,  0.0260,  0.0368,  0.0735,\n",
       "        -0.0700,  0.0067,  0.0257,  0.0366, -0.0665,  0.0026,  0.1077,  0.0221,\n",
       "        -0.0551, -0.0046, -0.0401,  0.0270,  0.0398,  0.0040,  0.0764,  0.0195,\n",
       "         0.0727,  0.0070, -0.0913,  0.0205, -0.0157, -0.0561, -0.0043,  0.0315,\n",
       "         0.0511,  0.0232,  0.0493,  0.0084,  0.0404,  0.0917,  0.0832, -0.0080,\n",
       "        -0.0139, -0.0806,  0.0046,  0.0782,  0.0227,  0.0231,  0.0360,  0.0291,\n",
       "        -0.0110,  0.0085, -0.0171, -0.0080, -0.0111,  0.0020, -0.0211, -0.1468,\n",
       "        -0.0414, -0.0335,  0.0122,  0.0304, -0.0750, -0.0682, -0.0460,  0.0578,\n",
       "         0.0180, -0.0765, -0.0547,  0.0265,  0.0140, -0.0045, -0.0731, -0.0657,\n",
       "         0.0041, -0.0095,  0.0324,  0.0097,  0.0364,  0.0195,  0.0292, -0.0109,\n",
       "        -0.0317, -0.0445, -0.0362,  0.0013,  0.0646,  0.0355,  0.0035, -0.0554,\n",
       "        -0.0697,  0.0055,  0.0069, -0.0025, -0.0678,  0.0583, -0.0248,  0.0153,\n",
       "         0.0176,  0.0521, -0.0165,  0.0059, -0.0054, -0.0202, -0.0224,  0.0492,\n",
       "        -0.0309, -0.0431,  0.0251, -0.0604,  0.0092,  0.0238,  0.0335,  0.0489,\n",
       "         0.0063,  0.0245,  0.0021,  0.0423,  0.0769,  0.0811, -0.0267, -0.0203,\n",
       "        -0.0670, -0.0666, -0.0017,  0.0279, -0.0420, -0.0170,  0.0094,  0.0314,\n",
       "         0.0024, -0.0871, -0.0528,  0.0643, -0.0793, -0.0184,  0.0555,  0.0270,\n",
       "        -0.0252,  0.0301,  0.0962, -0.0501, -0.0012, -0.0114, -0.0744, -0.0275,\n",
       "        -0.0246, -0.0125,  0.0327,  0.0328, -0.0115, -0.0133,  0.0086, -0.0697,\n",
       "        -0.0561, -0.0306, -0.0804,  0.0070,  0.0544, -0.1287, -0.0128, -0.0131,\n",
       "         0.0245,  0.0110,  0.0202, -0.1034,  0.0011, -0.0097, -0.0081, -0.0427,\n",
       "         0.0247, -0.0312, -0.0221,  0.0683, -0.0098,  0.0067,  0.0462,  0.0394,\n",
       "         0.0990,  0.1101, -0.0583,  0.0017, -0.0527,  0.0364, -0.0218,  0.0019,\n",
       "         0.0259,  0.0930, -0.0779, -0.0498, -0.0122,  0.1172, -0.0123, -0.0550,\n",
       "        -0.0323, -0.0007, -0.0201, -0.0196, -0.0131,  0.0502, -0.0819, -0.0135,\n",
       "        -0.0132, -0.0176, -0.0405,  0.0488, -0.0032,  0.0302,  0.0630, -0.0252,\n",
       "        -0.0388,  0.0538, -0.0222, -0.0206,  0.0291, -0.0115,  0.0325,  0.0044,\n",
       "         0.0468, -0.0524, -0.0173, -0.0233,  0.0619, -0.0138, -0.0912,  0.0303,\n",
       "         0.0047,  0.0537, -0.0378,  0.0460, -0.0509, -0.0191, -0.0488, -0.0145,\n",
       "         0.0464, -0.0466,  0.0320,  0.0270,  0.0137, -0.0698, -0.0873,  0.0272,\n",
       "        -0.0609, -0.0610, -0.0127,  0.0669, -0.0459, -0.0006, -0.0447, -0.0182],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1(model.flatten(model_thi_output))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0221, 0.0131],\n",
       "         [0.0125, 0.0159]],\n",
       "\n",
       "        [[0.0345, 0.0462],\n",
       "         [0.0547, 0.0554]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.0000, 0.0000]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_thi_output[0][perm3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.weight.shape)\n",
    "print(model.conv2.weight.shape)\n",
    "print(model.conv3.weight.shape)\n",
    "print(model.fc1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "randimg = torch.rand(1,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm1 = torch.tensor(np.random.permutation(np.linspace(0,63,64)),dtype=torch.int)\n",
    "perm2 = torch.tensor(np.random.permutation(np.linspace(0,127,128)),dtype=torch.int)\n",
    "#perm3 = torch.tensor(np.random.permutation(np.linspace(0,255,256)),dtype=torch.int)\n",
    "perm3 = torch.tensor(np.linspace(0,255,256),dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_perm1 = [0] * len(perm1)\n",
    "for i, p in enumerate(perm1):\n",
    "    reverse_perm1[p] = i\n",
    "    \n",
    "reverse_perm2 = [0] * len(perm2)\n",
    "for i, p in enumerate(perm2):\n",
    "    reverse_perm2[p] = i\n",
    "\n",
    "reverse_perm3 = [0] * len(perm3)\n",
    "for i, p in enumerate(perm3):\n",
    "    reverse_perm3[p] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_model = permute_weights(model, perm1, perm2, perm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output after model.conv1 is correct!\n"
     ]
    }
   ],
   "source": [
    "# Both outputs should be of dimension 1x64x64x64, which stands for (batch size)x(channels)x(height)x(width)\n",
    "model_first_output = model.pool(F.relu(model.conv1(randimg)))\n",
    "perm_model_first_output = model.pool(F.relu(permuted_model.conv1(randimg)))\n",
    "\n",
    "all_close = True\n",
    "for (idx, perm) in enumerate(reverse_perm1):\n",
    "    all_close &= torch.all(torch.isclose(model_first_output[0][idx][0], perm_model_first_output[0][reverse_perm1[idx]][0],atol=1e-06))\n",
    "\n",
    "if all_close:\n",
    "    print(\"The output after model.conv1 is correct!\")\n",
    "else:\n",
    "    print(\"!!!INCORRECT!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!INCORRECT!!!\n"
     ]
    }
   ],
   "source": [
    "# Both outputs should be of dimension 1x64x64x64, which stands for (batch size)x(channels)x(height)x(width)\n",
    "model_sec_output = model.pool(F.relu(model.conv2(model_first_output)))\n",
    "perm_model_sec_output = model.pool(F.relu(permuted_model.conv2(model_first_output)))\n",
    "\n",
    "all_close = True\n",
    "for (idx, perm) in enumerate(reverse_perm2):\n",
    "    all_close &= torch.all(torch.isclose(model_sec_output[0][idx][0], perm_model_sec_output[0][reverse_perm2[idx]][0],atol=1e-06))\n",
    "\n",
    "if all_close:\n",
    "    print(\"The output after model.conv2 is correct!\")\n",
    "else:\n",
    "    print(\"!!!INCORRECT!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!INCORRECT!!!\n"
     ]
    }
   ],
   "source": [
    "# Both outputs should be of dimension 1x64x64x64, which stands for (batch size)x(channels)x(height)x(width)\n",
    "model_thi_output = model.pool(F.relu(model.conv3(model_sec_output)))\n",
    "perm_model_thi_output = model.pool(F.relu(permuted_model.conv3(perm_model_sec_output)))\n",
    "\n",
    "all_close = True\n",
    "for (idx, perm) in enumerate(reverse_perm3):\n",
    "    all_close &= torch.all(torch.isclose(model_thi_output[0][idx][0], perm_model_thi_output[0][reverse_perm3[idx]][0],atol=1e-06))\n",
    "\n",
    "if all_close:\n",
    "    print(\"The output after model.conv3 is correct!\")\n",
    "else:\n",
    "    print(\"!!!INCORRECT!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0422, 0.0565, 0.0798, 0.0418, 0.1041, 0.1389],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sec_output[0][perm2[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3498, 0.3379, 0.4253, 0.4661, 0.4123, 0.3815],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_model_sec_output[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 6, 6])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sec_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 6, 6])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_model_sec_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7308e-02,  6.8467e-02,  4.9798e-02,  2.1670e-03,  2.5407e-02,\n",
       "         -5.3186e-02, -4.7419e-03,  8.4272e-02,  9.2971e-02,  3.1007e-02,\n",
       "         -1.1453e-01, -7.8077e-03, -3.2241e-03, -7.9952e-02, -5.4270e-02,\n",
       "         -4.3620e-02, -3.9943e-02, -1.0245e-01, -9.0447e-02, -3.1020e-02,\n",
       "          7.3450e-02,  3.5338e-02, -9.3056e-02,  5.6870e-03, -5.1347e-02,\n",
       "         -7.4622e-02, -4.2496e-02,  2.7777e-02,  5.4548e-02,  1.6968e-02,\n",
       "          5.4765e-02, -3.7355e-02, -9.5297e-02,  6.2199e-02,  1.5989e-02,\n",
       "          3.2986e-02, -6.5759e-02,  5.6070e-02, -6.6707e-02,  2.8574e-02,\n",
       "          1.3708e-02, -2.5447e-02,  8.9721e-05, -5.6360e-02, -7.3957e-02,\n",
       "          6.7465e-02,  2.3382e-02,  7.4468e-03,  3.7261e-02,  2.8554e-02,\n",
       "         -1.3031e-02,  1.1721e-02,  6.2530e-02,  7.3760e-02, -5.7035e-03,\n",
       "          3.2922e-02,  4.7257e-02,  4.4316e-02, -8.0760e-02,  2.1118e-02,\n",
       "          3.1636e-03,  1.0655e-01,  3.2337e-03, -8.4182e-03, -7.0871e-02,\n",
       "          5.3775e-02,  3.3382e-02,  7.5237e-02,  6.6655e-03, -3.2385e-02,\n",
       "         -4.1092e-02,  4.2529e-02, -9.3101e-03,  3.1537e-02,  2.4891e-02,\n",
       "          5.4425e-02, -7.9704e-02,  8.7862e-03, -7.3208e-02,  2.6446e-02,\n",
       "          6.1303e-02, -3.0820e-02,  2.3743e-02,  7.0461e-02, -3.8911e-02,\n",
       "         -6.9869e-02, -1.0877e-02, -8.7505e-02,  1.0097e-01,  4.4264e-02,\n",
       "         -3.6567e-03, -9.3727e-02, -3.6666e-05,  2.0737e-02,  9.4285e-03,\n",
       "          3.9653e-04,  5.3271e-02,  4.8671e-02,  8.4294e-02,  4.4352e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(randimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7308e-02,  6.8467e-02,  4.9798e-02,  2.1670e-03,  2.5407e-02,\n",
       "         -5.3186e-02, -4.7419e-03,  8.4272e-02,  9.2971e-02,  3.1007e-02,\n",
       "         -1.1453e-01, -7.8077e-03, -3.2241e-03, -7.9952e-02, -5.4270e-02,\n",
       "         -4.3620e-02, -3.9943e-02, -1.0245e-01, -9.0447e-02, -3.1020e-02,\n",
       "          7.3450e-02,  3.5338e-02, -9.3056e-02,  5.6870e-03, -5.1347e-02,\n",
       "         -7.4622e-02, -4.2496e-02,  2.7777e-02,  5.4548e-02,  1.6968e-02,\n",
       "          5.4765e-02, -3.7355e-02, -9.5297e-02,  6.2199e-02,  1.5989e-02,\n",
       "          3.2986e-02, -6.5759e-02,  5.6070e-02, -6.6707e-02,  2.8574e-02,\n",
       "          1.3708e-02, -2.5447e-02,  8.9720e-05, -5.6360e-02, -7.3957e-02,\n",
       "          6.7465e-02,  2.3382e-02,  7.4468e-03,  3.7261e-02,  2.8554e-02,\n",
       "         -1.3031e-02,  1.1721e-02,  6.2530e-02,  7.3760e-02, -5.7035e-03,\n",
       "          3.2922e-02,  4.7257e-02,  4.4316e-02, -8.0760e-02,  2.1118e-02,\n",
       "          3.1636e-03,  1.0655e-01,  3.2337e-03, -8.4182e-03, -7.0871e-02,\n",
       "          5.3775e-02,  3.3382e-02,  7.5237e-02,  6.6655e-03, -3.2385e-02,\n",
       "         -4.1092e-02,  4.2529e-02, -9.3101e-03,  3.1537e-02,  2.4891e-02,\n",
       "          5.4425e-02, -7.9704e-02,  8.7862e-03, -7.3208e-02,  2.6446e-02,\n",
       "          6.1303e-02, -3.0820e-02,  2.3743e-02,  7.0461e-02, -3.8911e-02,\n",
       "         -6.9869e-02, -1.0877e-02, -8.7505e-02,  1.0097e-01,  4.4264e-02,\n",
       "         -3.6566e-03, -9.3727e-02, -3.6659e-05,  2.0737e-02,  9.4285e-03,\n",
       "          3.9653e-04,  5.3271e-02,  4.8671e-02,  8.4294e-02,  4.4352e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_model(randimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
