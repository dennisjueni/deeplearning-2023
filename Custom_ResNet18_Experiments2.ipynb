{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ldiazbone/miniconda3/envs/ML/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/ldiazbone/miniconda3/envs/ML/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import infrastructure as inf\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch \n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_to_filter_per_layer(models):\n",
    "    print(\"Using\",len(models),\"models to create filters\")\n",
    "    filters_per_layer = {}\n",
    "    \n",
    "    filters_per_layer[\"0\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"0\"].append(model.conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"1\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"1\"].append(model.layer1[0].conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"2\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"2\"].append(model.layer1[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"3\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"3\"].append(model.layer1[1].conv1.weight.data.cpu().numpy())\n",
    "    \n",
    "    filters_per_layer[\"4\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"4\"].append(model.layer1[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"5\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"5\"].append(model.layer2[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"6\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"6\"].append(model.layer2[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"7\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"7\"].append(model.layer2[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"8\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"8\"].append(model.layer2[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"9\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"9\"].append(model.layer3[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"10\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"10\"].append(model.layer3[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"11\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"11\"].append(model.layer3[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"12\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"12\"].append(model.layer3[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"13\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"13\"].append(model.layer4[0].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"14\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"14\"].append(model.layer4[0].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"15\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"15\"].append(model.layer4[1].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    filters_per_layer[\"16\"] = []\n",
    "    for model in models:\n",
    "        filters_per_layer[\"16\"].append(model.layer4[1].conv2.weight.data.cpu().numpy())\n",
    "\n",
    "    for key in filters_per_layer.keys():\n",
    "        filters_per_layer[key] = np.array(filters_per_layer[key])\n",
    "\n",
    "    return filters_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def dft(weights):\n",
    "    ffts = []\n",
    "    for i in range(weights.shape[0]):\n",
    "        ffts.append(np.fft.fft2(weights[i]))\n",
    "    ffts = np.array(ffts)\n",
    "    return np.real(ffts), np.imag(ffts)\n",
    "\n",
    "def inverse_dft(cluster_results):\n",
    "    weights = []\n",
    "    for i in range(cluster_results.shape[0]):\n",
    "        weights.append(np.fft.ifft2(cluster_results[i]))\n",
    "    weights = np.array(weights)\n",
    "    return np.real(weights), np.imag(weights)\n",
    "\n",
    "# filters = filters_per_layer[layer_key]\n",
    "def clustering_fourier_single_layer(filters, num_clusters):\n",
    "    filters_reshaped = np.reshape(filters, (filters.shape[0]*filters.shape[1]*filters.shape[2],filters.shape[3],filters.shape[4]))\n",
    "    dfts, _ = dft(filters_reshaped)\n",
    "    dfts_reshaped = np.reshape(dfts,(dfts.shape[0],dfts.shape[1]*dfts.shape[2]))\n",
    "    kmeans = KMeans(n_clusters = num_clusters, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(dfts_reshaped)\n",
    "    final_weights, _ = inverse_dft(np.reshape(kmeans.cluster_centers_, (dfts.shape[0],dfts.shape[1],dfts.shape[2])))\n",
    "    return cluster_labels, final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 49)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = np.load(\"saved_filters_for_key0.npy\")\n",
    "filters_reshaped = np.reshape(filters, (filters.shape[0]*filters.shape[1]*filters.shape[2],filters.shape[3],filters.shape[4]))\n",
    "dfts, _ = dft(filters_reshaped)\n",
    "np.reshape(dfts,(dfts.shape[0],dfts.shape[1]*dfts.shape[2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gabor_filter(size, sigma, theta, Lambda, psi, gamma):\n",
    "    \"\"\"\n",
    "    Generates a Gabor filter with given parameters.\n",
    "    :param size: Size of the filter (size x size).\n",
    "    :param sigma: Standard deviation of the Gaussian envelope.\n",
    "    :param theta: Orientation of the Gabor filter.\n",
    "    :param Lambda: Wavelength of the sinusoidal factor.\n",
    "    :param psi: Phase offset.\n",
    "    :param gamma: Spatial aspect ratio.\n",
    "    :return: Gabor filter as a 2D array.\n",
    "    \"\"\"\n",
    "    sigma_x = sigma\n",
    "    sigma_y = sigma / gamma\n",
    "\n",
    "    # Prepare grid in x and y\n",
    "    x = np.linspace(-size // 2, size // 2, size)\n",
    "    y = np.linspace(-size // 2, size // 2, size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    # Rotation\n",
    "    x_theta = x * np.cos(theta) + y * np.sin(theta)\n",
    "    y_theta = -x * np.sin(theta) + y * np.cos(theta)\n",
    "\n",
    "    gb = np.exp(-.5 * (x_theta ** 2 / sigma_x ** 2 + y_theta ** 2 / sigma_y ** 2)) * np.cos(2 * np.pi / Lambda * x_theta + psi)\n",
    "    return gb\n",
    "\n",
    "def initialize_model_gabor():\n",
    "    \n",
    "    model = resnet18(num_classes=10)\n",
    "\n",
    "    for i in range(model.conv1.out_channels):  # Output channels of conv1\n",
    "        np.random.seed(i)   #generate the same filters for each model but random inside of a model\n",
    "        sigma = np.random.uniform(1.5, 2.5)  # Random sigma (Standard deviation of the Gaussian envelope)\n",
    "        theta = np.random.uniform(0, np.pi)  # Random theta (Orientation of the Gabor filter)\n",
    "        Lambda = np.random.uniform(2, 13)  # Random Lambda (wavelength)\n",
    "        psi = np.random.uniform(0, 2*np.pi)   # Random psi (phase offset)\n",
    "        gamma = np.random.uniform(0.9, 1.1)  # Random gamma (aspect ratio)\n",
    "        for j in range(model.conv1.in_channels):  # Input channels of conv1\n",
    "            gabor_filter = generate_gabor_filter(7, sigma, theta, Lambda, psi, gamma)\n",
    "            model.conv1.weight.data[i, j, :, :] = torch.from_numpy(gabor_filter)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
