{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "import infrastructure as inf\n",
    "\n",
    "device = inf.device\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_ResNet18 import custom_resnet_18\n",
    "\n",
    "models = {}\n",
    "\n",
    "max_superclass = 3\n",
    "\n",
    "for i in range(0,max_superclass):\n",
    "    for j in range(i+1,max_superclass):\n",
    "        path = \"results_training_run1/models/model_{}_{}.pt\".format(i,j)\n",
    "        models[(i,j)] = custom_resnet_18(num_classes=10).to(device)\n",
    "        models[(i,j)].load_state_dict(torch.load(path, map_location=device))\n",
    "        models[(i,j)].eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock2(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock2(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(models[(0,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering to Initialization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting into buckets\n",
    "\n",
    "We start by taking the 64 filters of the first model as the basis for our clustering. We then sort the filters for each model into the buckets based on their cosine similarity, but in such a way that in each round only 1 filters is added to a bucket. We then take the avg filter from each bucket and use it as the basis for the next layer of clustering. We repeat this process until we have 64 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(cluster_group, vec):\n",
    "    total_distance = 0\n",
    "\n",
    "    for cluster_vec in cluster_group:\n",
    "        \n",
    "        # Reshape vectors to 2D arrays as required by cosine_similarity function\n",
    "        vec1 = np.array(cluster_vec).reshape(1, -1)\n",
    "        vec2 = np.array(vec).reshape(1, -1)\n",
    "\n",
    "        # Calculate cosine similarity and convert to cosine distance\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Since cosine_similarity returns a 2D array, we take the first element\n",
    "        total_distance += distance[0][0]\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_clustering_groups(filters_per_model, num_filters_per_model):\n",
    "    filter_vectors = []\n",
    "    \n",
    "    num_models = len(filters_per_model)\n",
    "    \n",
    "    for idx, filters in enumerate(filters_per_model):\n",
    "        \n",
    "        filter_vectors.append([])\n",
    "\n",
    "        for i in range(filters.shape[0]):\n",
    "            # Flatten and normalize the filter\n",
    "            filter_vec = filters[i].flatten()\n",
    "            filter_vec /= np.linalg.norm(filter_vec)\n",
    "            filter_vectors[idx].append(filter_vec)\n",
    "            \n",
    "    new_groups = [[] for _ in range(num_filters_per_model)]\n",
    "    permutations = [[] for _ in range(num_models)]\n",
    "    \n",
    "    # Now we go through all models and assign filters in a greedy manner\n",
    "    for i in range(num_models):\n",
    "        model_filters = filter_vectors[i]\n",
    "        used_indices = set()\n",
    "        \n",
    "        # We go through every group of the new clusters and assign the min distance vector from the next model to it\n",
    "        for cluster_idx, cluster_group in enumerate(new_groups):\n",
    "            min_distance = float('inf')\n",
    "            selected_vector_idx = None\n",
    "            \n",
    "            for idx, vec in enumerate(model_filters):\n",
    "                dist = calculate_cosine_similarity(cluster_group, vec)\n",
    "                if dist < min_distance and idx not in used_indices:\n",
    "                    min_distance = dist\n",
    "                    selected_vector_idx = idx\n",
    "            \n",
    "            new_groups[cluster_idx].append(model_filters[selected_vector_idx])\n",
    "            used_indices.add(selected_vector_idx)\n",
    "            permutations[i].append(selected_vector_idx)\n",
    "\n",
    "    return new_groups, permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_clustering(filters, num_input_channels, num_filters, kernel_size, include_permutations=False):\n",
    "    \n",
    "    clustered_groups, permutations = get_custom_clustering_groups(filters, num_filters)\n",
    "    \n",
    "    filter_vectors = np.array(clustered_groups)\n",
    "    average_filters = [np.mean(np.array(vectors), axis=0) for vectors in filter_vectors]\n",
    "    \n",
    "    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n",
    "    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n",
    "    \n",
    "    if include_permutations:\n",
    "        return torch_shaped_average_filters, permutations\n",
    "    else:\n",
    "        return torch_shaped_average_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(filters, num_input_channels, num_filters, kernel_size):\n",
    "    \n",
    "    filter_vectors = []\n",
    "        \n",
    "    num_models = len(filters)\n",
    "    \n",
    "    for idx, model_filters in enumerate(filters):\n",
    "        \n",
    "        filter_vectors.append([])\n",
    "\n",
    "        for i in range(model_filters.shape[0]):\n",
    "            # Flatten and normalize the filter\n",
    "            filter_vec = model_filters[i].flatten()\n",
    "            filter_vec /= np.linalg.norm(filter_vec)\n",
    "            filter_vectors[idx].append(filter_vec)\n",
    "    \n",
    "    filter_vectors = np.vstack(filter_vectors)\n",
    "    kmeans = KMeans(n_clusters=num_filters, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(filter_vectors)\n",
    "\n",
    "    average_filters = []\n",
    "\n",
    "    for cluster_num in range(num_filters):\n",
    "        \n",
    "        # Find indices where the cluster label matches the current cluster number\n",
    "        indices = np.where(cluster_labels == cluster_num)[0]\n",
    "\n",
    "        # Filters belonging to the current cluster\n",
    "        cluster_filters = filter_vectors[indices]\n",
    "\n",
    "        # Compute the average filter for this cluster\n",
    "        average_filter = np.mean(cluster_filters, axis=0)\n",
    "        average_filters.append(average_filter)\n",
    "        \n",
    "    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n",
    "    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return torch_shaped_average_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_bias(permuted_model_bias, perm):\n",
    "    permuted_model_bias.weight = torch.nn.Parameter(permuted_model_bias.weight[perm])\n",
    "    permuted_model_bias.bias = torch.nn.Parameter(permuted_model_bias.bias[perm])\n",
    "    permuted_model_bias.running_mean = permuted_model_bias.running_mean[perm]\n",
    "    permuted_model_bias.running_var = permuted_model_bias.running_var[perm]\n",
    "\n",
    "def permute_layer(permuted_layer, permuted_layer_next, permuted_bias, perm = None):\n",
    "    if not perm is None: \n",
    "        permuted_layer.weight = torch.nn.Parameter(permuted_layer.weight[perm])\n",
    "        permute_bias(permuted_bias, perm)\n",
    "        permuted_layer_next.weight = torch.nn.Parameter(permuted_layer_next.weight.transpose(0,1)[perm].transpose(0,1))\n",
    "    \n",
    "\n",
    "def permute_weights(model, permutation):\n",
    "    \n",
    "     with torch.no_grad():\n",
    "        permuted_model = copy.deepcopy(model)\n",
    "        permute_layer(permuted_model.conv1, permuted_model.layer1[0].conv1, permuted_model.bn1, permutation)\n",
    "        return permuted_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initialized_model_custom(models_dict):\n",
    "    pretrained_models = [model for model in models_dict.values()] \n",
    "    final_model = custom_resnet_18(num_classes=10).to(device)\n",
    "\n",
    "    #Apply first layer clustering techniques\n",
    "    trained_filters_layer1 = [model.conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer1 = kmeans_clustering(filters=trained_filters_layer1,\n",
    "                                               num_input_channels=3,\n",
    "                                               num_filters=64,\n",
    "                                               kernel_size=7)\n",
    "    \n",
    "    # Apply second layer clustering techniques\n",
    "    trained_filters_layer2 = [model.layer1[0].conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer2 = kmeans_clustering(filters=trained_filters_layer2,\n",
    "                                               num_input_channels=64,\n",
    "                                               num_filters=64,\n",
    "                                               kernel_size=3)\n",
    "    \n",
    "    # Apply third layer clustering techniques\n",
    "    trained_filters_layer3 = [model.layer1[0].conv2.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer3 = kmeans_clustering(filters=trained_filters_layer3,\n",
    "                                               num_input_channels=64,\n",
    "                                               num_filters=64,\n",
    "                                               kernel_size=3)\n",
    "    \n",
    "    # Apply fourth layer clustering techniques\n",
    "    trained_filters_layer4 = [model.layer1[1].conv1.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer4 = kmeans_clustering(filters=trained_filters_layer4,\n",
    "                                               num_input_channels=64,\n",
    "                                               num_filters=64,\n",
    "                                               kernel_size=3)\n",
    "    \n",
    "    # Apply fifth layer clustering techniques\n",
    "    trained_filters_layer5 = [model.layer1[1].conv2.weight.data.cpu().numpy() for model in models]\n",
    "    sampled_filters_layer5 = kmeans_clustering(filters=trained_filters_layer5,\n",
    "                                               num_input_channels=64,\n",
    "                                               num_filters=64,\n",
    "                                               kernel_size=3)\n",
    "    \n",
    "    \n",
    "    final_model.conv1.weight.data = sampled_filters_layer1\n",
    "    final_model.layer1[0].conv1.weight.data = sampled_filters_layer2\n",
    "    final_model.layer1[0].conv2.weight.data = sampled_filters_layer3\n",
    "    final_model.layer1[1].conv1.weight.data = sampled_filters_layer4\n",
    "    final_model.layer1[1].conv2.weight.data = sampled_filters_layer5\n",
    "    \n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(final_model, random_model, loaders, num_epochs):\n",
    "    \n",
    "    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=0.01)\n",
    "    random_optimizer = torch.optim.Adam(random_model.parameters(), lr=0.01)\n",
    "    \n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    cluster_tracked_params = inf.train(final_model) # fill out the rest here. \n",
    "    random_tracked_params = inf.train(random_model)\n",
    "\n",
    "    return cluster_tracked_params, random_tracked_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 2555.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done initializing models,start training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m loaders \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaders import requieres ffcv import\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m models_without_keys \u001b[38;5;241m=\u001b[39m [model \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mvalues()] \n\u001b[0;32m---> 25\u001b[0m \u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_without_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mcompare_models\u001b[0;34m(models, loaders, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m random_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(random_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     10\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 12\u001b[0m cluster_tracked_params \u001b[38;5;241m=\u001b[39m \u001b[43minf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fill out the rest here. \u001b[39;00m\n\u001b[1;32m     13\u001b[0m random_tracked_params \u001b[38;5;241m=\u001b[39m inf\u001b[38;5;241m.\u001b[39mtrain(random_model)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cluster_tracked_params, random_tracked_params\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'loaders'"
     ]
    }
   ],
   "source": [
    "# might want to enable batch tracking later, now only the epochs are tracked\n",
    "#loaders = inf.make_dataloaders(batch_size=128)[0]\n",
    "loaders = \"Loaders import requieres ffcv import\"\n",
    "custom_clustered_model = create_initialized_model_custom(models)\n",
    "random_model = custom_resnet_18(num_classes=10).to(device)\n",
    "\n",
    "compare_models(custom_clustered_model, random_model, loaders,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering for RGB seperately\n",
    "\n",
    "Each filters outputs are used for different filters in the next layer, these dependencies can be visualized as a graph. We start clustering on single 3d dimensional filters (for the first layer) and then having a prob distribution over the clusters for each filter in the next layer. We then use a pointer network to predict the cluster for each filter in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def clustering_kmeans(models, mode):\n",
    "    # mode can be one of the following\n",
    "    # \"euclid\" \n",
    "    # \"cosine\"\n",
    "    # \"fft\" (not implemented yet)\n",
    "    \n",
    "    # go over all models and extract the first layer, and store it in a list\n",
    "    first_layer_weights = []\n",
    "    for model in models:\n",
    "        first_layer_weights.append(models[model].conv1.weight.data.cpu().numpy())\n",
    "\n",
    "    # flatten the list\n",
    "    first_layer_weights = np.array(first_layer_weights)\n",
    "    print(first_layer_weights.shape)\n",
    "        \n",
    "    # combine the first two dimensions of this np array\n",
    "    first_layer_weights = first_layer_weights.reshape(-1, *first_layer_weights.shape[2:])\n",
    "    print(first_layer_weights.shape)\n",
    "    \n",
    "    # flatten the list \n",
    "    filters_r = first_layer_weights[:,0,:,:]\n",
    "    filters_r = filters_r.reshape(filters_r.shape[0],-1)\n",
    "    filters_g = first_layer_weights[:,1,:,:]\n",
    "    filters_g = filters_g.reshape(filters_g.shape[0],-1)\n",
    "    filters_b = first_layer_weights[:,2,:,:]\n",
    "    filters_b = filters_b.reshape(filters_b.shape[0],-1)\n",
    "\n",
    "    print(filters_r.shape)\n",
    "\n",
    "    # make this a list of values from 30 to 300 in steps of 10\n",
    "    choices_num_clusters = list(range(1,100,2))\n",
    "\n",
    "     # use elbow method to find the best number of clusters\n",
    "    \n",
    "    loss_list = {}\n",
    "\n",
    "    for n_clusters in choices_num_clusters: \n",
    "        kmeans_r = KMeans(n_clusters=n_clusters, n_init='auto',)\n",
    "        cluster_labels = kmeans_r.fit_predict(filters_r)\n",
    "        loss_list[(0,n_clusters)] = kmeans_r.inertia_\n",
    "\n",
    "        # repeat for green\n",
    "        kmeans_g = KMeans(n_clusters=n_clusters, n_init='auto',)\n",
    "        cluster_labels = kmeans_g.fit_predict(filters_g)\n",
    "        loss_list[(1,n_clusters)] = kmeans_g.inertia_\n",
    "\n",
    "        # repeat for blue\n",
    "        kmeans_b = KMeans(n_clusters=n_clusters, n_init='auto',)\n",
    "        cluster_labels = kmeans_b.fit_predict(filters_b)\n",
    "        loss_list[(2,n_clusters)] = kmeans_b.inertia_\n",
    "\n",
    "\n",
    "    print(loss_list)\n",
    "    # make 1x3 plot to plot the loss for each color channel\n",
    "    fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    # plot the loss for red\n",
    "    data_r = [loss_list[(0,n_clusters)] for n_clusters in choices_num_clusters]\n",
    "    axs[0].plot(choices_num_clusters,data_r)\n",
    "    # add y label\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    # add x label\n",
    "    axs[0].set_xlabel('Number of clusters')\n",
    "    # add title\n",
    "    axs[0].set_title('Loss vs number of clusters for red channel')\n",
    "\n",
    "    # plot the loss for green\n",
    "    data_g = [loss_list[(1,n_clusters)] for n_clusters in choices_num_clusters]\n",
    "    axs[1].plot(choices_num_clusters,data_g)\n",
    "    # add y label\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    # add x label\n",
    "    axs[1].set_xlabel('Number of clusters')\n",
    "    # add title\n",
    "    axs[1].set_title('Loss vs number of clusters for green channel')\n",
    "\n",
    "    # plot the loss for blue\n",
    "    data_b = [loss_list[(2,n_clusters)] for n_clusters in choices_num_clusters]\n",
    "    axs[2].plot(choices_num_clusters,data_b)\n",
    "    # add y label\n",
    "    axs[2].set_ylabel('Loss')\n",
    "    # add x label\n",
    "    axs[2].set_xlabel('Number of clusters')\n",
    "    # add title\n",
    "    axs[2].set_title('Loss vs number of clusters for blue channel')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#clusters = clustering_kmeans(models,mode='euclid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
