{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7177737,"sourceType":"datasetVersion","datasetId":4148191}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/customresnet-dl23')\n\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copy\nimport random\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom Custom_ResNet18 import *\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-20T13:23:37.308213Z","iopub.execute_input":"2023-12-20T13:23:37.308693Z","iopub.status.idle":"2023-12-20T13:23:42.368752Z","shell.execute_reply.started":"2023-12-20T13:23:37.308666Z","shell.execute_reply":"2023-12-20T13:23:42.367417Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading Partial Data","metadata":{}},{"cell_type":"code","source":"def create_partial_datasets(classes):\n    # Load CIFAR-100 dataset\n    train_data = torchvision.datasets.CIFAR100(\n        root='data',\n        train=True,\n        transform=torchvision.transforms.ToTensor(),\n        download=True\n    )\n    test_data = torchvision.datasets.CIFAR100(\n        root='data',\n        train=False,\n        transform=torchvision.transforms.ToTensor(),\n    )\n    \n    class_names = [train_data.classes[i] for i in classes]\n    \n    # Mapping for labels to start at 0\n    mapping = {old: new for new, old in enumerate(classes)}\n\n    # Filter and map the training dataset\n    train_idx = [i for i, label in enumerate(train_data.targets) if label in classes]\n    train_data.data = train_data.data[train_idx]\n    train_data.targets = torch.tensor([mapping[train_data.targets[i]] for i in train_idx])\n\n    # Filter and map the testing dataset\n    test_idx = [i for i, label in enumerate(test_data.targets) if label in classes]\n    test_data.data = test_data.data[test_idx]\n    test_data.targets = torch.tensor([mapping[test_data.targets[i]] for i in test_idx])\n    \n\n    return train_data, test_data, class_names\n","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:24:12.847183Z","iopub.execute_input":"2023-12-20T13:24:12.848122Z","iopub.status.idle":"2023-12-20T13:24:12.857636Z","shell.execute_reply.started":"2023-12-20T13:24:12.848086Z","shell.execute_reply":"2023-12-20T13:24:12.856750Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"classes_per_set = 10\n\ntrain_classes = []\ntest_classes = []\nstring_labels = []\n\nfor i in range(0, 100, classes_per_set):\n    class_indices = [num for num in range(i, i + classes_per_set)]\n    train, test, class_names = create_partial_datasets(class_indices)\n    train_classes.append(train)\n    test_classes.append(test)\n    string_labels.append(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:24:13.386624Z","iopub.execute_input":"2023-12-20T13:24:13.386989Z","iopub.status.idle":"2023-12-20T13:24:33.275508Z","shell.execute_reply.started":"2023-12-20T13:24:13.386961Z","shell.execute_reply":"2023-12-20T13:24:33.274688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:02<00:00, 56483800.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/cifar-100-python.tar.gz to data\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"loaders = []\n\nfor idx, (train_data, test_data) in enumerate(zip(train_classes, test_classes)):\n    loaders.append({\n        'train': torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n        'test': torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1),\n    })","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:26:40.718208Z","iopub.execute_input":"2023-12-20T13:26:40.719101Z","iopub.status.idle":"2023-12-20T13:26:40.725344Z","shell.execute_reply.started":"2023-12-20T13:26:40.719065Z","shell.execute_reply":"2023-12-20T13:26:40.724455Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Training Models","metadata":{}},{"cell_type":"code","source":"def evaluate(model, test_loader):\n\n    model.eval()\n    \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            test_output = model(images)\n            pred_y = torch.max(test_output, 1)[1]\n            correct += (pred_y == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:26:45.356870Z","iopub.execute_input":"2023-12-20T13:26:45.357220Z","iopub.status.idle":"2023-12-20T13:26:45.363940Z","shell.execute_reply.started":"2023-12-20T13:26:45.357189Z","shell.execute_reply":"2023-12-20T13:26:45.362804Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train(num_epochs, train_loader, model, optimizer, loss_func, test_loader=None):\n    \n    test_accuracies = []\n    \n    model.train()\n    \n    for epoch in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n                        \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            output = model(images)\n            loss = loss_func(output, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n        # We check the test accuracy after every single epoch\n        if test_loader is not None:\n            accuracy = evaluate(model, test_loader)\n            model.train()\n\n            test_accuracies.append(accuracy)\n        \n        if (epoch+1) % 5 == 0: \n            print(f\"  Finished epoch {epoch+1}\")\n    \n    return test_accuracies","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:26:45.858536Z","iopub.execute_input":"2023-12-20T13:26:45.859402Z","iopub.status.idle":"2023-12-20T13:26:45.869547Z","shell.execute_reply.started":"2023-12-20T13:26:45.859358Z","shell.execute_reply":"2023-12-20T13:26:45.868449Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"num_epochs = 20\nnum_models = 10\n\nmodels = [custom_resnet_18(num_classes=10).to(device) for _ in range(num_models)]\noptimizers = [torch.optim.Adam(model.parameters(), lr=0.01) for model in models]\nloss_func = nn.CrossEntropyLoss()\n\nfor i, model in enumerate(models):\n    print(f\"Training model [{i+1}/{num_models}] on dataset {i}\")\n    \n    optimizer = optimizers[i]\n    # Use modulo 9 to make sure that we do not train on final dataset\n    train(num_epochs, loaders[i%9][\"train\"], model, optimizer, loss_func)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:26:46.398674Z","iopub.execute_input":"2023-12-20T13:26:46.399036Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training model [1/10] on dataset 0\n  Finished epoch 5\n  Finished epoch 10\n  Finished epoch 15\n  Finished epoch 20\nTraining model [2/10] on dataset 1\n  Finished epoch 5\n  Finished epoch 10\n  Finished epoch 15\n  Finished epoch 20\nTraining model [3/10] on dataset 2\n  Finished epoch 5\n  Finished epoch 10\n  Finished epoch 15\n  Finished epoch 20\nTraining model [4/10] on dataset 3\n  Finished epoch 5\n  Finished epoch 10\n  Finished epoch 15\n  Finished epoch 20\nTraining model [5/10] on dataset 4\n  Finished epoch 5\n  Finished epoch 10\n  Finished epoch 15\n","output_type":"stream"}]},{"cell_type":"code","source":"for i, model in enumerate(models):\n    acc = evaluate(model, loaders[i%9][\"test\"])\n    print(f\"Testing model [{i+1}/{num_models}]: {acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Clustering","metadata":{}},{"cell_type":"code","source":"def calculate_cosine_similarity(cluster_group, vec):\n    total_distance = 0\n\n    for cluster_vec in cluster_group:\n        \n        # Reshape vectors to 2D arrays as required by cosine_similarity function\n        vec1 = np.array(cluster_vec).reshape(1, -1)\n        vec2 = np.array(vec).reshape(1, -1)\n\n        # Calculate cosine similarity and convert to cosine distance\n        similarity = cosine_similarity(vec1, vec2)\n        distance = 1 - similarity\n\n        # Since cosine_similarity returns a 2D array, we take the first element\n        total_distance += distance[0][0]\n\n    return total_distance","metadata":{"execution":{"iopub.status.busy":"2023-12-20T09:53:52.938290Z","iopub.execute_input":"2023-12-20T09:53:52.939272Z","iopub.status.idle":"2023-12-20T09:53:52.946360Z","shell.execute_reply.started":"2023-12-20T09:53:52.939227Z","shell.execute_reply":"2023-12-20T09:53:52.945371Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"def get_custom_clustering_groups(filters_per_model, num_filters_per_model):\n    filter_vectors = []\n    \n    num_models = len(filters_per_model)\n    \n    for idx, filters in enumerate(filters_per_model):\n        \n        filter_vectors.append([])\n\n        for i in range(filters.shape[0]):\n            # Flatten and normalize the filter\n            filter_vec = filters[i].flatten()\n            filter_vec /= np.linalg.norm(filter_vec)\n            filter_vectors[idx].append(filter_vec)\n            \n    new_groups = [[] for _ in range(num_filters_per_model)]\n    permutations = [[] for _ in range(num_models)]\n    \n    # Now we go through all models and assign filters in a greedy manner\n    for i in range(num_models):\n        model_filters = filter_vectors[i]\n        used_indices = set()\n        \n        # We go through every group of the new clusters and assign the min distance vector from the next model to it\n        for cluster_idx, cluster_group in enumerate(new_groups):\n            min_distance = float('inf')\n            selected_vector_idx = None\n            \n            for idx, vec in enumerate(model_filters):\n                dist = calculate_cosine_similarity(cluster_group, vec)\n                if dist < min_distance and idx not in used_indices:\n                    min_distance = dist\n                    selected_vector_idx = idx\n            \n            new_groups[cluster_idx].append(model_filters[selected_vector_idx])\n            used_indices.add(selected_vector_idx)\n            permutations[i].append(selected_vector_idx)\n\n    return new_groups, permutations","metadata":{"execution":{"iopub.status.busy":"2023-12-20T10:29:04.004488Z","iopub.execute_input":"2023-12-20T10:29:04.005398Z","iopub.status.idle":"2023-12-20T10:29:04.014810Z","shell.execute_reply.started":"2023-12-20T10:29:04.005364Z","shell.execute_reply":"2023-12-20T10:29:04.013887Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"def custom_clustering(filters, num_input_channels, num_filters, kernel_size, include_permutations=False):\n    \n    clustered_groups, permutations = get_custom_clustering_groups(filters, num_filters)\n    \n    filter_vectors = np.array(clustered_groups)\n    average_filters = [np.mean(np.array(vectors), axis=0) for vectors in filter_vectors]\n    \n    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n    \n    if include_permutations:\n        return torch_shaped_average_filters, permutations\n    else:\n        return torch_shaped_average_filters","metadata":{"execution":{"iopub.status.busy":"2023-12-20T10:29:04.203971Z","iopub.execute_input":"2023-12-20T10:29:04.204279Z","iopub.status.idle":"2023-12-20T10:29:04.211171Z","shell.execute_reply.started":"2023-12-20T10:29:04.204253Z","shell.execute_reply":"2023-12-20T10:29:04.210186Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"## GMM","metadata":{}},{"cell_type":"code","source":"def gmm_clustering(filters, num_input_channels, num_filters, kernel_size):\n    filter_vectors = []\n    \n    for model_filters in filters:\n        for i in range(model_filters.shape[0]):\n            # Flatten and normalize the filter\n            filter_vec = model_filters[i].flatten()\n            filter_vec /= np.linalg.norm(filter_vec)\n            filter_vectors.append(filter_vec)\n    \n    filter_vectors = np.array(filter_vectors)\n    \n    # Use Gaussian Mixture Models for clustering\n    gmm = GaussianMixture(n_components=num_filters, init_params='kmeans')\n    gmm.fit(filter_vectors)\n    cluster_labels = gmm.predict(filter_vectors)\n\n    average_filters = []\n\n    for cluster_num in range(num_filters):\n        # Filters belonging to the current cluster\n        cluster_filters = filter_vectors[cluster_labels == cluster_num]\n\n        # Compute the average filter for this cluster\n        average_filter = np.mean(cluster_filters, axis=0)\n        average_filters.append(average_filter)\n\n    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n    \n    return torch_shaped_average_filters","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:08:41.198601Z","iopub.execute_input":"2023-12-20T13:08:41.198994Z","iopub.status.idle":"2023-12-20T13:08:41.208825Z","shell.execute_reply.started":"2023-12-20T13:08:41.198962Z","shell.execute_reply":"2023-12-20T13:08:41.207535Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## K-Means","metadata":{}},{"cell_type":"code","source":"def kmeans_clustering(filters, num_input_channels, num_filters, kernel_size):\n    \n    filter_vectors = []\n        \n    num_models = len(filters)\n    \n    for idx, model_filters in enumerate(filters):\n        \n        filter_vectors.append([])\n\n        for i in range(model_filters.shape[0]):\n            # Flatten and normalize the filter\n            filter_vec = model_filters[i].flatten()\n            filter_vec /= np.linalg.norm(filter_vec)\n            filter_vectors[idx].append(filter_vec)\n    \n    filter_vectors = np.vstack(filter_vectors)\n    kmeans = KMeans(n_clusters=num_filters, n_init='auto')\n    cluster_labels = kmeans.fit_predict(filter_vectors)\n\n    average_filters = []\n\n    for cluster_num in range(num_filters):\n        \n        # Find indices where the cluster label matches the current cluster number\n        indices = np.where(cluster_labels == cluster_num)[0]\n\n        # Filters belonging to the current cluster\n        cluster_filters = filter_vectors[indices]\n\n        # Compute the average filter for this cluster\n        average_filter = np.mean(cluster_filters, axis=0)\n        average_filters.append(average_filter)\n        \n    shaped_average_filters = np.array(average_filters).reshape((num_filters, num_input_channels, kernel_size, kernel_size))\n    torch_shaped_average_filters = torch.tensor(shaped_average_filters, dtype=torch.float32).to(device)\n    \n    return torch_shaped_average_filters","metadata":{"execution":{"iopub.status.busy":"2023-12-20T10:09:34.169691Z","iopub.execute_input":"2023-12-20T10:09:34.170125Z","iopub.status.idle":"2023-12-20T10:09:34.180620Z","shell.execute_reply.started":"2023-12-20T10:09:34.170093Z","shell.execute_reply":"2023-12-20T10:09:34.179481Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"## Permutation","metadata":{}},{"cell_type":"code","source":"def permute_bias(permuted_model_bias, perm):\n    permuted_model_bias.weight = torch.nn.Parameter(permuted_model_bias.weight[perm])\n    permuted_model_bias.bias = torch.nn.Parameter(permuted_model_bias.bias[perm])\n    permuted_model_bias.running_mean = permuted_model_bias.running_mean[perm]\n    permuted_model_bias.running_var = permuted_model_bias.running_var[perm]\n\ndef permute_layer(permuted_layer, permuted_layer_next, permuted_bias, perm = None):\n    if not perm is None: \n        permuted_layer.weight = torch.nn.Parameter(permuted_layer.weight[perm])\n        permute_bias(permuted_bias, perm)\n        permuted_layer_next.weight = torch.nn.Parameter(permuted_layer_next.weight.transpose(0,1)[perm].transpose(0,1))","metadata":{"execution":{"iopub.status.busy":"2023-12-20T10:25:42.969553Z","iopub.execute_input":"2023-12-20T10:25:42.969925Z","iopub.status.idle":"2023-12-20T10:25:42.977114Z","shell.execute_reply.started":"2023-12-20T10:25:42.969896Z","shell.execute_reply":"2023-12-20T10:25:42.976109Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def permute_weights(model, permutation):\n    \n     with torch.no_grad():\n        permuted_model = copy.deepcopy(model)\n        permute_layer(permuted_model.conv1, permuted_model.layer1[0].conv1, permuted_model.bn1, permutation)\n        return permuted_model","metadata":{"execution":{"iopub.status.busy":"2023-12-20T10:25:43.358550Z","iopub.execute_input":"2023-12-20T10:25:43.358913Z","iopub.status.idle":"2023-12-20T10:25:43.366018Z","shell.execute_reply.started":"2023-12-20T10:25:43.358883Z","shell.execute_reply":"2023-12-20T10:25:43.364607Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"## Initialization","metadata":{}},{"cell_type":"code","source":"def create_initialized_model(models):\n    final_model = custom_resnet_18(num_classes=10).to(device)\n    \n    #Apply first layer clustering techniques\n    trained_filters_layer1 = [model.conv1.weight.data.cpu().numpy() for model in models]\n    sampled_filters_layer1 = gmm_clustering(filters=trained_filters_layer1,\n                                               num_input_channels=3,\n                                               num_filters=64,\n                                               kernel_size=7)\n    \n    # Apply second layer clustering techniques\n    trained_filters_layer2 = [model.layer1[0].conv1.weight.data.cpu().numpy() for model in models]\n    sampled_filters_layer2 = gmm_clustering(filters=trained_filters_layer2,\n                                               num_input_channels=64,\n                                               num_filters=64,\n                                               kernel_size=3)\n    \n    # Apply third layer clustering techniques\n    trained_filters_layer3 = [model.layer1[0].conv2.weight.data.cpu().numpy() for model in models]\n    sampled_filters_layer3 = gmm_clustering(filters=trained_filters_layer3,\n                                               num_input_channels=64,\n                                               num_filters=64,\n                                               kernel_size=3)\n    \n    # Apply fourth layer clustering techniques\n    trained_filters_layer4 = [model.layer1[1].conv1.weight.data.cpu().numpy() for model in models]\n    sampled_filters_layer4 = gmm_clustering(filters=trained_filters_layer4,\n                                               num_input_channels=64,\n                                               num_filters=64,\n                                               kernel_size=3)\n    \n    # Apply fifth layer clustering techniques\n    trained_filters_layer5 = [model.layer1[1].conv2.weight.data.cpu().numpy() for model in models]\n    sampled_filters_layer5 = gmm_clustering(filters=trained_filters_layer5,\n                                               num_input_channels=64,\n                                               num_filters=64,\n                                               kernel_size=3)\n    \n    \n    final_model.conv1.weight.data = sampled_filters_layer1\n    final_model.layer1[0].conv1.weight.data = sampled_filters_layer2\n    final_model.layer1[0].conv2.weight.data = sampled_filters_layer3\n    final_model.layer1[1].conv1.weight.data = sampled_filters_layer4\n    final_model.layer1[1].conv2.weight.data = sampled_filters_layer5\n    \n    \n    return final_model","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:08:49.070947Z","iopub.execute_input":"2023-12-20T13:08:49.071718Z","iopub.status.idle":"2023-12-20T13:08:49.082775Z","shell.execute_reply.started":"2023-12-20T13:08:49.071683Z","shell.execute_reply":"2023-12-20T13:08:49.081724Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compare_models(models, loaders, num_epochs):\n    \n    random_model = custom_resnet_18(num_classes=10).to(device)\n    final_model = create_initialized_model(models)\n    \n    random_optimizer = torch.optim.Adam(random_model.parameters(), lr=0.01)\n    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=0.01)\n    \n    loss_func = torch.nn.CrossEntropyLoss()\n    \n    final_accuracies = train(num_epochs, loaders[9][\"train\"], final_model, final_optimizer, loss_func,\n                           test_loader=loaders[9][\"test\"])\n    random_accuracies = train(num_epochs, loaders[9][\"train\"], random_model, random_optimizer, loss_func,\n                              test_loader=loaders[9][\"test\"])\n\n\n    return random_accuracies, final_accuracies","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:09:49.966837Z","iopub.execute_input":"2023-12-20T13:09:49.967300Z","iopub.status.idle":"2023-12-20T13:09:49.976299Z","shell.execute_reply.started":"2023-12-20T13:09:49.967264Z","shell.execute_reply":"2023-12-20T13:09:49.975272Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_average_score(num_iterations, models, loaders, plot_result=True):\n    num_epochs = 20\n    \n    random_cumulative_accuracies = []\n    final_cumulative_accuracies = []\n\n    for i in range(num_iterations):\n        random_accuracies, final_accuracies = compare_models(models, loaders, num_epochs)\n        \n        print(f\"Random accuracies after 1st epoch: {random_accuracies[0]}, after 2nd epoch: {random_accuracies[1]}, after 3rd epoch {random_accuracies[2]}\")\n        print(f\"Final accuracies after 1st epoch: {final_accuracies[0]}, after 2nd epoch: {final_accuracies[1]}, after 3rd epoch {final_accuracies[2]}\")\n\n        if not random_cumulative_accuracies:\n            random_cumulative_accuracies = random_accuracies\n            final_cumulative_accuracies = final_accuracies\n        else:\n            # Accumulate the accuracies\n            random_cumulative_accuracies = [x + y for x, y in zip(random_cumulative_accuracies, random_accuracies)]\n            final_cumulative_accuracies = [x + y for x, y in zip(final_cumulative_accuracies, final_accuracies)]\n\n    # Compute the average accuracies\n    random_average_accuracies = [x / num_iterations for x in random_cumulative_accuracies]\n    final_average_accuracies = [x / num_iterations for x in final_cumulative_accuracies]\n    \n    if plot_result:\n        plt.figure(figsize=(10, 6))\n        plt.plot(random_average_accuracies, label='Random Model - Average Accuracy')\n        plt.plot(final_average_accuracies, label='Our Model - Average Accuracy')\n        plt.title('Model Comparison - Average Accuracy per Epoch')\n        plt.xlabel('Epochs')\n        plt.ylabel('Average Accuracy')\n        plt.legend()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:09:50.484904Z","iopub.execute_input":"2023-12-20T13:09:50.485852Z","iopub.status.idle":"2023-12-20T13:09:50.495741Z","shell.execute_reply.started":"2023-12-20T13:09:50.485816Z","shell.execute_reply":"2023-12-20T13:09:50.494639Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"get_average_score(10, models, loaders)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:09:50.497378Z","iopub.execute_input":"2023-12-20T13:09:50.497748Z","iopub.status.idle":"2023-12-20T13:10:14.139805Z","shell.execute_reply.started":"2023-12-20T13:09:50.497720Z","shell.execute_reply":"2023-12-20T13:10:14.138332Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_average_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mget_average_score\u001b[0;34m(num_iterations, models, loaders, plot_result)\u001b[0m\n\u001b[1;32m      5\u001b[0m final_cumulative_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m----> 8\u001b[0m     random_accuracies, final_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom accuracies after 1st epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_accuracies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, after 2nd epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_accuracies[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, after 3rd epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom_accuracies[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal accuracies after 1st epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, after 2nd epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracies[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, after 3rd epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracies[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mcompare_models\u001b[0;34m(models, loaders, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m final_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(final_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      9\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 11\u001b[0m final_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m random_accuracies \u001b[38;5;241m=\u001b[39m train(num_epochs, loaders[\u001b[38;5;241m9\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], random_model, random_optimizer, loss_func,\n\u001b[1;32m     14\u001b[0m                           test_loader\u001b[38;5;241m=\u001b[39mloaders[\u001b[38;5;241m9\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m random_accuracies, final_accuracies\n","Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, train_loader, model, optimizer, loss_func, test_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(output, labels)\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We check the test accuracy after every single epoch\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"],"ename":"RuntimeError","evalue":"cuDNN error: CUDNN_STATUS_MAPPING_ERROR","output_type":"error"}]},{"cell_type":"code","source":"models[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}